{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 - Information Extraction\n",
    "\n",
    "\n",
    "This week, we move from arbitrary textual classification to the use of computation and linguistic models to parse precise claims from documents. Rather than focusing on simply the *ideas* in a corpus, here we focus on understanding and extracting its precise *claims*. This process involves a sequential pipeline of classifying and structuring tokens from text, each of which generates potentially useful data for the content analyst. Steps in this process, which we examine in this notebook, include: 1) tagging words by their part of speech (POS) to reveal the linguistic role they play in the sentence (e.g., Verb, Noun, Adjective, etc.); 2) tagging words as named entities (NER) such as places or organizations; 3) structuring or \"parsing\" sentences into nested phrases that are local to, describe or depend on one another; and 4) extracting informational claims from those phrases, like the Subject-Verb-Object (SVO) triples we extract here. While much of this can be done directly in the python package NLTK that we introduced in week 2, here we use NLTK bindings to the Stanford NLP group's open software, written in Java. Try typing a sentence into the online version [here](http://nlp.stanford.edu:8080/corenlp/) to get a sense of its potential. It is superior in performance to NLTK's implementations, but takes time to run, and so for these exercises we will parse and extract information for a very small text corpus. Of course, for final projects that draw on these tools, we encourage you to install the software on your own machines or shared servers at the university (RCC, SSRC) in order to perform these operations on much more text. \n",
    "\n",
    "For this notebook we will be using the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "import lucem_illud #pip install -U git+git://github.com/Computational-Content-Analysis-2018/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "#For NLP\n",
    "import nltk\n",
    "\n",
    "import numpy as np #For arrays\n",
    "import pandas #Gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "\n",
    "#Displays the graphs\n",
    "import graphviz #You also need to install the command line graphviz\n",
    "\n",
    "#These are from the standard library\n",
    "import os.path\n",
    "import zipfile\n",
    "import subprocess\n",
    "import io\n",
    "import tempfile\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to run this _once_ to download everything, you will also need [Java 1.8+](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html) if you are using Windows or MacOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting downloads, this will take 5-10 minutes\n",
      "../stanford-NLP/parser already exists, skipping download\n",
      "../stanford-NLP/ner already exists, skipping download\n",
      "../stanford-NLP/postagger already exists, skipping download\n",
      "../stanford-NLP/core already exists, skipping download\n",
      "Done setting up the Stanford NLP collection\n"
     ]
    }
   ],
   "source": [
    "lucem_illud.setupStanfordNLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to have stanford-NLP setup before importing, so we are doing the import here. IF you have stanford-NLP working, you can import at the beginning like you would with any other library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/nltk/tag/stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/nltk/tag/stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import lucem_illud.stanford as stanford"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Information Extraction is a module packaged within the Stanford Core NLP package, but it is not yet supported by `nltk`. As a result, we have defining our own `lucem_illud` function that runs the Stanford Core NLP java code right here. For other projects, it is often useful to use Java or other programs (in C, C++) within a python workflow, and this is an example. `stanford.openIE()` takes in a string or list of strings and then produces as output all the subject, verb, object (SVO) triples Stanford Corenlp can find, as a DataFrame. You can do this through links to the Stanford Core NLP project that we provide here, or play with their interface directly (in the penultimate cell of this notebook), which produces data in \"pretty graphics\" like this example parsing of the first sentence in the \"Shooting of Trayvon Martin\" Wikipedia article:\n",
    "\n",
    "![Output 1](../data/stanford_core1.png)\n",
    "![Output 2](../data/stanford_core2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will illustrate these tools on some *very* short examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw the elephant in my pajamas.\n",
      "The quick brown fox jumped over the lazy dog.\n",
      "While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.\n",
      "Trayvon Benjamin Martin was an African American from Miami Gardens, Florida, who, at 17 years old, was fatally shot by George Zimmerman, a neighborhood watch volunteer, in Sanford, Florida.\n",
      "Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo\n"
     ]
    }
   ],
   "source": [
    "text = ['I saw the elephant in my pajamas.', 'The quick brown fox jumped over the lazy dog.', 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.', 'Trayvon Benjamin Martin was an African American from Miami Gardens, Florida, who, at 17 years old, was fatally shot by George Zimmerman, a neighborhood watch volunteer, in Sanford, Florida.', 'Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo']\n",
    "tokenized_text = [nltk.word_tokenize(t) for t in text]\n",
    "print('\\n'.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech (POS) tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In POS tagging, we classify each word by its semantic role in a sentence. The Stanford POS tagger uses the [Penn Treebank tag set]('http://repository.upenn.edu/cgi/viewcontent.cgi?article=1603&context=cis_reports') to POS tag words from input sentences. As discussed in the second assignment, this is a relatively precise tagset, which allows more informative tags, and also more opportunities to err :-).\n",
    "\n",
    "|#. |Tag |Description |\n",
    "|---|----|------------|\n",
    "|1.\t|CC\t|Coordinating conjunction\n",
    "|2.\t|CD\t|Cardinal number\n",
    "|3.\t|DT\t|Determiner\n",
    "|4.\t|EX\t|Existential there\n",
    "|5.\t|FW\t|Foreign word\n",
    "|6.\t|IN\t|Preposition or subordinating conjunction\n",
    "|7.\t|JJ\t|Adjective\n",
    "|8.\t|JJR|\tAdjective, comparative\n",
    "|9.\t|JJS|\tAdjective, superlative\n",
    "|10.|\tLS\t|List item marker\n",
    "|11.|\tMD\t|Modal\n",
    "|12.|\tNN\t|Noun, singular or mass\n",
    "|13.|\tNNS\t|Noun, plural\n",
    "|14.|\tNNP\t|Proper noun, singular\n",
    "|15.|\tNNPS|\tProper noun, plural\n",
    "|16.|\tPDT\t|Predeterminer\n",
    "|17.|\tPOS\t|Possessive ending\n",
    "|18.|\tPRP\t|Personal pronoun\n",
    "|19.|\tPRP\\$|\tPossessive pronoun\n",
    "|20.|\tRB\t|Adverb\n",
    "|21.|\tRBR\t|Adverb, comparative\n",
    "|22.|\tRBS\t|Adverb, superlative\n",
    "|23.|\tRP\t|Particle\n",
    "|24.|\tSYM\t|Symbol\n",
    "|25.|\tTO\t|to\n",
    "|26.|\tUH\t|Interjection\n",
    "|27.|\tVB\t|Verb, base form\n",
    "|28.|\tVBD\t|Verb, past tense\n",
    "|29.|\tVBG\t|Verb, gerund or present participle\n",
    "|30.|\tVBN\t|Verb, past participle\n",
    "|31.|\tVBP\t|Verb, non-3rd person singular present\n",
    "|32.|\tVBZ\t|Verb, 3rd person singular present\n",
    "|33.|\tWDT\t|Wh-determiner\n",
    "|34.|\tWP\t|Wh-pronoun\n",
    "|35.|\tWP$\t|Possessive wh-pronoun\n",
    "|36.|\tWRB\t|Wh-adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('I', 'PRP'), ('saw', 'VBD'), ('the', 'DT'), ('elephant', 'NN'), ('in', 'IN'), ('my', 'PRP$'), ('pajamas', 'NNS'), ('.', '.')], [('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')], [('While', 'IN'), ('in', 'IN'), ('France', 'NNP'), (',', ','), ('Christine', 'NNP'), ('Lagarde', 'NNP'), ('discussed', 'VBD'), ('short-term', 'JJ'), ('stimulus', 'NN'), ('efforts', 'NNS'), ('in', 'IN'), ('a', 'DT'), ('recent', 'JJ'), ('interview', 'NN'), ('with', 'IN'), ('the', 'DT'), ('Wall', 'NNP'), ('Street', 'NNP'), ('Journal', 'NNP'), ('.', '.')], [('Trayvon', 'NNP'), ('Benjamin', 'NNP'), ('Martin', 'NNP'), ('was', 'VBD'), ('an', 'DT'), ('African', 'NNP'), ('American', 'NNP'), ('from', 'IN'), ('Miami', 'NNP'), ('Gardens', 'NNP'), (',', ','), ('Florida', 'NNP'), (',', ','), ('who', 'WP'), (',', ','), ('at', 'IN'), ('17', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('was', 'VBD'), ('fatally', 'RB'), ('shot', 'VBN'), ('by', 'IN'), ('George', 'NNP'), ('Zimmerman', 'NNP'), (',', ','), ('a', 'DT'), ('neighborhood', 'NN'), ('watch', 'NN'), ('volunteer', 'NN'), (',', ','), ('in', 'IN'), ('Sanford', 'NNP'), (',', ','), ('Florida', 'NNP'), ('.', '.')], [('Buffalo', 'NNP'), ('buffalo', 'NN'), ('Buffalo', 'NNP'), ('buffalo', 'NN'), ('buffalo', 'NN'), ('buffalo', 'NN'), ('Buffalo', 'NNP'), ('buffalo', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "pos_sents = stanford.postTagger.tag_sents(tokenized_text)\n",
    "print(pos_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks quite good. Now we will try POS tagging with a somewhat larger corpus. We consider a few of the top posts from the reddit data we used last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redditDF = pandas.read_csv('../data/reddit.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grabbing the 10 highest scoring posts and tokenizing the sentences. Once again, notice that we aren't going to do any kind of stemming this week (although *semantic* normalization may be performed where we translate synonyms into the same focal word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>over_18</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>goldie-gold</td>\n",
       "      <td>False</td>\n",
       "      <td>12650</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>This just happened...  So, I had a laptop syst...</td>\n",
       "      <td>Engineer is doing drugs!! No. No they aren't.</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[This, just, happened, ...], [So, ,, I, had, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TheDroolinFool</td>\n",
       "      <td>False</td>\n",
       "      <td>13152</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>Another tale from the out of hours IT desk... ...</td>\n",
       "      <td>\"I need you to fix Google Bing immediately!\"</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[Another, tale, from, the, out, of, hours, IT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clickity_clickity</td>\n",
       "      <td>False</td>\n",
       "      <td>13404</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>[Part 1](http://www.reddit.com/r/talesfromtech...</td>\n",
       "      <td>Jack, the Worst End User, Part 4</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[[, Part, 1, ], (, http, :, //www.reddit.com/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SECGaz</td>\n",
       "      <td>False</td>\n",
       "      <td>13724</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>&gt; $Me  - Hello, IT.   &gt; $Usr - Hi, I am still ...</td>\n",
       "      <td>Hi, I am still off sick but I am not.</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[&gt;, $, Me, -, Hello, ,, IT, .], [&gt;, $, Usr, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>guitarsdontdance</td>\n",
       "      <td>False</td>\n",
       "      <td>14089</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>So my story starts on what was a normal day ta...</td>\n",
       "      <td>\"Don't bother sending a tech, I'll be dead by ...</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[So, my, story, starts, on, what, was, a, nor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              author  over_18  score                subreddit  \\\n",
       "4        goldie-gold    False  12650  Tales From Tech Support   \n",
       "3     TheDroolinFool    False  13152  Tales From Tech Support   \n",
       "2  Clickity_clickity    False  13404  Tales From Tech Support   \n",
       "1             SECGaz    False  13724  Tales From Tech Support   \n",
       "0   guitarsdontdance    False  14089  Tales From Tech Support   \n",
       "\n",
       "                                                text  \\\n",
       "4  This just happened...  So, I had a laptop syst...   \n",
       "3  Another tale from the out of hours IT desk... ...   \n",
       "2  [Part 1](http://www.reddit.com/r/talesfromtech...   \n",
       "1  > $Me  - Hello, IT.   > $Usr - Hi, I am still ...   \n",
       "0  So my story starts on what was a normal day ta...   \n",
       "\n",
       "                                               title  \\\n",
       "4      Engineer is doing drugs!! No. No they aren't.   \n",
       "3       \"I need you to fix Google Bing immediately!\"   \n",
       "2                   Jack, the Worst End User, Part 4   \n",
       "1              Hi, I am still off sick but I am not.   \n",
       "0  \"Don't bother sending a tech, I'll be dead by ...   \n",
       "\n",
       "                                                 url  \\\n",
       "4  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "3  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "2  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "1  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "0  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "\n",
       "                                           sentences  \n",
       "4  [[This, just, happened, ...], [So, ,, I, had, ...  \n",
       "3  [[Another, tale, from, the, out, of, hours, IT...  \n",
       "2  [[[, Part, 1, ], (, http, :, //www.reddit.com/...  \n",
       "1  [[>, $, Me, -, Hello, ,, IT, .], [>, $, Usr, -...  \n",
       "0  [[So, my, story, starts, on, what, was, a, nor...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redditTopScores = redditDF.sort_values('score')[-10:]\n",
    "redditTopScores['sentences'] = redditTopScores['text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "redditTopScores.index = range(len(redditTopScores) - 1, -1,-1) #Reindex to make things nice in the future\n",
    "redditTopScores[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redditTopScores['POS_sents'] = redditTopScores['sentences'].apply(lambda x: stanford.postTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9    [[(Last, JJ), (year, NN), (,, ,), (Help, NN), ...\n",
       "8    [[(First, JJ), (post, NN), (in, IN), (quite, R...\n",
       "7    [[([, NNP), (Original, NNP), (Post, NNP), (], ...\n",
       "6    [[(I, PRP), (witnessed, VBD), (this, DT), (ast...\n",
       "5    [[(I, PRP), (work, VBP), (Helpdesk, NNP), (for...\n",
       "4    [[(This, DT), (just, RB), (happened, VBN), (.....\n",
       "3    [[(Another, DT), (tale, NN), (from, IN), (the,...\n",
       "2    [[([, NNP), (Part, NNP), (1, CD), (], FW), ((,...\n",
       "1    [[(>, JJR), ($, $), (Me, PRP), (-, :), (Hello,...\n",
       "0    [[(So, RB), (my, PRP$), (story, NN), (starts, ...\n",
       "Name: POS_sents, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redditTopScores['POS_sents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And count the number of `NN` (nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('password', 21),\n",
       " ('(', 19),\n",
       " ('time', 14),\n",
       " (')', 14),\n",
       " ('lot', 12),\n",
       " ('computer', 12),\n",
       " ('life', 11),\n",
       " ('email', 11),\n",
       " ('**Genius**', 10),\n",
       " ('message', 9),\n",
       " ('**Me**', 9),\n",
       " ('system', 9),\n",
       " ('day', 9),\n",
       " ('call', 8),\n",
       " ('laptop', 8),\n",
       " ('office', 8),\n",
       " ('part', 8),\n",
       " ('today', 8),\n",
       " ('story', 8),\n",
       " ('user', 7)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'NN'\n",
    "targetCounts = {}\n",
    "for entry in redditTopScores['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the number of top verbs (`VB`)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 18),\n",
       " ('have', 17),\n",
       " ('get', 14),\n",
       " ('do', 11),\n",
       " ('change', 9),\n",
       " ('make', 8),\n",
       " ('know', 7),\n",
       " ('say', 7),\n",
       " ('help', 6),\n",
       " ('look', 6),\n",
       " ('tell', 6),\n",
       " ('send', 6),\n",
       " ('go', 5),\n",
       " ('work', 4),\n",
       " ('use', 4),\n",
       " ('receive', 4),\n",
       " ('thank', 4),\n",
       " ('feel', 4),\n",
       " ('want', 4),\n",
       " ('call', 4)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'VB'\n",
    "targetCounts = {}\n",
    "for entry in redditTopScores['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the adjectives that modify the word, \"computer\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'own', 'unrestricted'}\n"
     ]
    }
   ],
   "source": [
    "NTarget = 'JJ'\n",
    "Word = 'computer'\n",
    "NResults = set()\n",
    "for entry in redditTopScores['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for (ent1, kind1),(ent2,kind2) in zip(sentence[:-1], sentence[1:]):\n",
    "            if (kind1,ent2.lower())==(NTarget,Word):\n",
    "                NResults.add(ent1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(NResults)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating POS tagger\n",
    "\n",
    "We can check the POS tagger by running it on a manually tagged corpus and identifying a reasonable error metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " (',', ','),\n",
       " ('61', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " (',', ','),\n",
       " ('will', 'MD'),\n",
       " ('join', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('nonexecutive', 'JJ'),\n",
       " ('director', 'NN'),\n",
       " ('Nov.', 'NNP'),\n",
       " ('29', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeBank = nltk.corpus.treebank\n",
    "treeBank.tagged_sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeBank.sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stanfordTags = stanford.postTagger.tag_sents(treeBank.sents()[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compare the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: Dutch  \tStanford: JJ\tTreebank: NNP\n",
      "Word: publishing  \tStanford: NN\tTreebank: VBG\n",
      "Word: used  \tStanford: VBD\tTreebank: VBN\n",
      "Word: more  \tStanford: JJR\tTreebank: RBR\n",
      "Word: ago  \tStanford: RB\tTreebank: IN\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: later  \tStanford: RB\tTreebank: JJ\n",
      "Word: New  \tStanford: NNP\tTreebank: JJ\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: more  \tStanford: JJR\tTreebank: RBR\n",
      "Word: ago  \tStanford: RB\tTreebank: IN\n",
      "Word: ago  \tStanford: RB\tTreebank: IN\n",
      "Word: replaced  \tStanford: VBD\tTreebank: VBN\n",
      "Word: more  \tStanford: JJR\tTreebank: JJ\n",
      "Word: expected  \tStanford: VBD\tTreebank: VBN\n",
      "Word: study  \tStanford: VBD\tTreebank: VBP\n",
      "Word: studied  \tStanford: VBD\tTreebank: VBN\n",
      "Word: industrialized  \tStanford: JJ\tTreebank: VBN\n",
      "Word: Lorillard  \tStanford: NNP\tTreebank: NN\n",
      "Word: found  \tStanford: VBD\tTreebank: VBN\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: rejected  \tStanford: VBD\tTreebank: VBN\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: poured  \tStanford: VBN\tTreebank: VBD\n",
      "Word: in  \tStanford: IN\tTreebank: RP\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "The Precision is 96.547%\n"
     ]
    }
   ],
   "source": [
    "NumDiffs = 0\n",
    "for sentIndex in range(len(stanfordTags)):\n",
    "    for wordIndex in range(len(stanfordTags[sentIndex])):\n",
    "        if stanfordTags[sentIndex][wordIndex][1] != treeBank.tagged_sents()[sentIndex][wordIndex][1]:\n",
    "            if treeBank.tagged_sents()[sentIndex][wordIndex][1] != '-NONE-':\n",
    "                print(\"Word: {}  \\tStanford: {}\\tTreebank: {}\".format(stanfordTags[sentIndex][wordIndex][0], stanfordTags[sentIndex][wordIndex][1], treeBank.tagged_sents()[sentIndex][wordIndex][1]))\n",
    "                NumDiffs += 1\n",
    "total = sum([len(s) for s in stanfordTags])\n",
    "print(\"The Precision is {:.3f}%\".format((total-NumDiffs)/total * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that the stanford POS tagger is quite good. Nevertheless, for a 20 word sentence, we only have a 66% chance ($1-.96^{20}$) of tagging (and later parsing) it correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform POS tagging on a meaningful (but modest) subset of a corpus associated with your final project. Examine the list of words associated with at least three different parts of speech. Consider conditional frequencies (e.g., adjectives associated with nouns of interest or adverbs with verbs of interest). What do these distributions suggest about your corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2005, the Council of Indonesian Islamic Scholars (MUI) issued a fatwa, or religious opinion, that condemned the idea of pluralism, liberalism, and secularism as running counter to Islamic teachings in terms of doctrine and acts of worship. In regards to human interactions, the fatwa stated that in a society with religious pluralism the Muslim community should assume an exclusivist attitude and only interact with non-Muslims to the extent of not inflicting harm. However, what ‘harm’ really means was not clearly defined in the fatwa and is left open to interpretation. Islamic social movement organizations across the Indonesian political spectrum, such as the vigilantes of the Islamic Defenders Front (FPI), regarded the fatwa as a vindication of their agendas for turning Indonesia into an Islamic state. Meanwhile, some other Islamic scholars such as those from Nahdlatul Ulama (NU) defied MUI’s fatwa and joined a broad coalition of pro-democracy activists who supported pluralism and religious tolerance. This rift in Indonesian Islamic social and political discourse has become a test for Indonesian democracy in its consolidation phase.\n",
    "\n",
    "I will look at two Indonesian Islamic social movement organizations, which are:\n",
    "\n",
    "1. **Islamic Defender Front (FPI)**, a vigilante group in favor of explicit Islamic agendas; and\n",
    "2. **Nahdlatul Ulama (NU)**, a traditionalist Muslim group, in favor of religious tolerance.\n",
    "\n",
    "There is a lack of organizational self-documentation, thus newspapers could reveal a lot about these social movement organizations as national news coverage signals activities deemed significant.\n",
    "\n",
    "<b>What:</b><br>\n",
    "Analyzing news articles between 2008 - 2018 on FPI and NU, Islamic social movement organizations in Indonesia.\n",
    "\n",
    "<b>Dataset:</b><br>\n",
    "**News articles scraped from Jakarta Post** (an Indonesian newspaper) that contains the word 'FPI' and 'NU' (the organization of interest).\n",
    "\n",
    "<b>Findings:</b><br>\n",
    "1) FPI seems to take a more contentious position towards the government and their engagement with both local and national government signals their political importance.<br>\n",
    "\n",
    "2) Words that define FPI (adjectives, JJ) are mixed between cooperative and contentious words. This may suggest instability of FPI's relations to other political actors such as the government. Meanwhile, for NU there are more verbs (VB) that imply cooperation rather than contention as we see in articles for FPI.<br>\n",
    "\n",
    "3) FPI and NU seems to be framed differently in media reporting, the word hard-line defines FPI as a group, while NU is defines as an 'Islamic' / 'Muslim' organization. FPI seems to be more contentious than NU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsDF = pandas.read_csv('all_data_cat_nu.csv', encoding='iso-8859-15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Islamic Defender Front (FPI)</h2>\n",
    "\n",
    "FIrst, let's look at FPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsFPI = newsDF.loc[newsDF['Topic'] == 'FPI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grabbing 10 random posts for FPI and tokenizing sentences in the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Article</th>\n",
       "      <th>Link</th>\n",
       "      <th>Topics</th>\n",
       "      <th>Year</th>\n",
       "      <th>Topic</th>\n",
       "      <th>is_NU</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sat, February 16 2013</td>\n",
       "      <td>Groups threaten to attack mosque, church</td>\n",
       "      <td>Two groups in Jakarta and Bekasi threatened on...</td>\n",
       "      <td>http://www.thejakartapost.com/news/2013/02/16/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>FPI</td>\n",
       "      <td>False</td>\n",
       "      <td>[[Two, groups, in, Jakarta, and, Bekasi, threa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thu, March 1 2012</td>\n",
       "      <td>Yogyakarta journalists speak out against violence</td>\n",
       "      <td>A number of local journalists from different m...</td>\n",
       "      <td>http://www.thejakartapost.com/news/2012/03/01/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012</td>\n",
       "      <td>FPI</td>\n",
       "      <td>False</td>\n",
       "      <td>[[A, number, of, local, journalists, from, dif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thu, October 7 2010</td>\n",
       "      <td>The tipping point, terrorism and crimes agains...</td>\n",
       "      <td>ItÍs mind-boggling that the National Police ha...</td>\n",
       "      <td>http://www.thejakartapost.com/news/2010/10/07/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010</td>\n",
       "      <td>FPI</td>\n",
       "      <td>False</td>\n",
       "      <td>[[ItÍs, mind-boggling, that, the, National, Po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tue, June 3 2008</td>\n",
       "      <td>Forum: FPI's attacks against protesters</td>\n",
       "      <td>That members of the FPI attacked activists of ...</td>\n",
       "      <td>http://www.thejakartapost.com/news/2008/06/03/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008</td>\n",
       "      <td>FPI</td>\n",
       "      <td>False</td>\n",
       "      <td>[[That, members, of, the, FPI, attacked, activ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tue, November 11, 2014</td>\n",
       "      <td>Home Minister to follow up Ahok'Ûªs letter on ...</td>\n",
       "      <td>Home Minister Tjahjo Kumolo has said that his ...</td>\n",
       "      <td>http://www.thejakartapost.com/news/2014/11/11/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "      <td>FPI</td>\n",
       "      <td>False</td>\n",
       "      <td>[[Home, Minister, Tjahjo, Kumolo, has, said, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Date                                              Title  \\\n",
       "4   Sat, February 16 2013           Groups threaten to attack mosque, church   \n",
       "3       Thu, March 1 2012  Yogyakarta journalists speak out against violence   \n",
       "2     Thu, October 7 2010  The tipping point, terrorism and crimes agains...   \n",
       "1        Tue, June 3 2008            Forum: FPI's attacks against protesters   \n",
       "0  Tue, November 11, 2014  Home Minister to follow up Ahok'Ûªs letter on ...   \n",
       "\n",
       "                                             Article  \\\n",
       "4  Two groups in Jakarta and Bekasi threatened on...   \n",
       "3  A number of local journalists from different m...   \n",
       "2  ItÍs mind-boggling that the National Police ha...   \n",
       "1  That members of the FPI attacked activists of ...   \n",
       "0  Home Minister Tjahjo Kumolo has said that his ...   \n",
       "\n",
       "                                                Link Topics  Year Topic  \\\n",
       "4  http://www.thejakartapost.com/news/2013/02/16/...    NaN  2013   FPI   \n",
       "3  http://www.thejakartapost.com/news/2012/03/01/...    NaN  2012   FPI   \n",
       "2  http://www.thejakartapost.com/news/2010/10/07/...    NaN  2010   FPI   \n",
       "1  http://www.thejakartapost.com/news/2008/06/03/...    NaN  2008   FPI   \n",
       "0  http://www.thejakartapost.com/news/2014/11/11/...    NaN  2014   FPI   \n",
       "\n",
       "   is_NU                                          sentences  \n",
       "4  False  [[Two, groups, in, Jakarta, and, Bekasi, threa...  \n",
       "3  False  [[A, number, of, local, journalists, from, dif...  \n",
       "2  False  [[ItÍs, mind-boggling, that, the, National, Po...  \n",
       "1  False  [[That, members, of, the, FPI, attacked, activ...  \n",
       "0  False  [[Home, Minister, Tjahjo, Kumolo, has, said, t...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FPIRandom = newsFPI.sample(n=10)\n",
    "FPIRandom['sentences'] = FPIRandom['Article'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "FPIRandom.index = range(len(FPIRandom) - 1, -1,-1) #Reindex to make things nice in the future\n",
    "FPIRandom[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FPIRandom['POS_sents'] = FPIRandom['sentences'].apply(lambda x: stanford.postTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9    [[(Muslims, NNPS), (nationwide, RB), (have, VB...\n",
       "8    [[(In, IN), (his, PRP$), (article, NN), (ñDonÍ...\n",
       "7    [[(An, DT), (intelligence, NN), (official, NN)...\n",
       "6    [[(A, DT), (police, NN), (official, NN), (said...\n",
       "5    [[(May, NNP), (30, CD), (,, ,), (p., NN), (30,...\n",
       "4    [[(Two, CD), (groups, NNS), (in, IN), (Jakarta...\n",
       "3    [[(A, DT), (number, NN), (of, IN), (local, JJ)...\n",
       "2    [[(ItÍs, NNS), (mind-boggling, JJ), (that, IN)...\n",
       "1    [[(That, DT), (members, NNS), (of, IN), (the, ...\n",
       "0    [[(Home, NNP), (Minister, NNP), (Tjahjo, NNP),...\n",
       "Name: POS_sents, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FPIRandom['POS_sents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at foreign word (FW) counts in the data.\n",
    "\n",
    "_As expected, as we are working with news articles with an Indonesian context there are many foreign words in the data that are mostly acronyms (ICMI, Babinsa, Kamtibnas, KUHP, FPI)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('(', 16),\n",
       " (')', 14),\n",
       " ('FPI', 8),\n",
       " ('[', 6),\n",
       " (']', 2),\n",
       " ('de', 2),\n",
       " ('facto', 2),\n",
       " ('non-pluralist', 1),\n",
       " ('Anies', 1),\n",
       " ('BaswedanÍs', 1),\n",
       " ('ñhave', 1),\n",
       " ('notsî', 1),\n",
       " ('in', 1),\n",
       " ('.î', 1),\n",
       " (\"Military'Ûªs\", 1),\n",
       " ('firebrand', 1),\n",
       " ('ICMI', 1),\n",
       " ('Babinsa', 1),\n",
       " ('Kamtibmas', 1),\n",
       " ('KUHP', 1)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'FW'\n",
    "targetCounts = {}\n",
    "for entry in FPIRandom['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the number of nouns (NN).\n",
    "\n",
    "_'Government' and 'Police' appears to be the the top nouns. What does the government has to do with FPI? Let's check Government's modifier._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('government', 19),\n",
       " ('police', 16),\n",
       " ('Front', 11),\n",
       " ('country', 10),\n",
       " ('attack', 9),\n",
       " (')', 8),\n",
       " ('group', 7),\n",
       " ('î', 7),\n",
       " (\"'Û\\x9d\", 7),\n",
       " ('freedom', 7),\n",
       " ('point', 7),\n",
       " ('organization', 6),\n",
       " ('incident', 6),\n",
       " ('public', 6),\n",
       " ('city', 6),\n",
       " ('administration', 6),\n",
       " ('security', 6),\n",
       " ('case', 6),\n",
       " ('mosque', 6),\n",
       " ('church', 5)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'NN'\n",
    "targetCounts = {}\n",
    "for entry in FPIRandom['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>'Indonesian', 'central' and 'local' modifies the word 'government'. It suggests that FPI is closely related to oth the local and national politics. Perhaps this infers that FPI important actor that influences both the local national polity, or at least they are engaged with government actors.\n",
    "\n",
    "This is an important point as accoding to Sidney Tarrow (2011) social movement is a vehicle for citizens who lack regular access to representative institutions to engage in politics by exerting power through contentious means. Its contention lies in collective ‘claim making' that may conflict with other political actors’ interests, usually the government. In this case FPI's interests might be in conflict with the government's.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Indonesian', 'central', 'Local'}\n"
     ]
    }
   ],
   "source": [
    "NTarget = 'JJ'\n",
    "Word = 'government'\n",
    "NResults = set()\n",
    "for entry in FPIRandom['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for (ent1, kind1),(ent2,kind2) in zip(sentence[:-1], sentence[1:]):\n",
    "            if (kind1,ent2.lower())==(NTarget,Word):\n",
    "                NResults.add(ent1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(NResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Top verbs (VB) in articles on FPI seems mixed between words that reflects cooperation such as 'talk', 'work' or 'join', but there are also more contentious words such as 'attack', 'stop' or 'enforce'.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 24),\n",
       " ('have', 5),\n",
       " ('follow', 4),\n",
       " ('report', 3),\n",
       " ('talk', 3),\n",
       " ('work', 3),\n",
       " ('engage', 3),\n",
       " ('help', 3),\n",
       " ('see', 3),\n",
       " ('do', 3),\n",
       " ('attack', 3),\n",
       " ('take', 3),\n",
       " ('ensure', 3),\n",
       " ('stop', 2),\n",
       " ('enforce', 2),\n",
       " ('become', 2),\n",
       " ('invite', 2),\n",
       " ('clarify', 2),\n",
       " ('fight', 2),\n",
       " ('join', 2)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'VB'\n",
    "targetCounts = {}\n",
    "for entry in FPIRandom['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>What about the adjectives that modify the word, \"group\" for articles in FPI? Interestingly, group (which refers to FPI) is most closely described (by adjective, JJ) as 'hard-line', which suggests that FPI might have an uncompromising character in their conducts with the government.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hard-line'}\n"
     ]
    }
   ],
   "source": [
    "NTarget = 'JJ'\n",
    "Word = 'group'\n",
    "NResults = set()\n",
    "for entry in FPIRandom['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for (ent1, kind1),(ent2,kind2) in zip(sentence[:-1], sentence[1:]):\n",
    "            if (kind1,ent2.lower())==(NTarget,Word):\n",
    "                NResults.add(ent1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(NResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Nahdlatul Ulama (NU)</h3>\n",
    "\n",
    "What about NU as the second Islamic social movement organization? Let's compare NU with FPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsNU = newsDF.loc[newsDF['Topic'] == 'NU']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grabbing 10 random articles for NU and tokenizing the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Article</th>\n",
       "      <th>Link</th>\n",
       "      <th>Topics</th>\n",
       "      <th>Year</th>\n",
       "      <th>Topic</th>\n",
       "      <th>is_NU</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mon, August 14 2017</td>\n",
       "      <td>Tension eases as school policy relaxed</td>\n",
       "      <td>The controversy surrounding the proposed five-...</td>\n",
       "      <td>http://www.thejakartapost.com/news/2017/08/14/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "      <td>NU</td>\n",
       "      <td>True</td>\n",
       "      <td>[[The, controversy, surrounding, the, proposed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wed, October 16, 2013</td>\n",
       "      <td>Ganjar Kurnia: The Sundanese culture keeper</td>\n",
       "      <td>Ganjar Kurnia. JP/Novia D. Rulistia As a Sunda...</td>\n",
       "      <td>http://www.thejakartapost.com/news/2013/10/16/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>NU</td>\n",
       "      <td>True</td>\n",
       "      <td>[[Ganjar, Kurnia, .], [JP/Novia, D., Rulistia,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fri, August 7, 2015</td>\n",
       "      <td>Reelected NU chairman pledges to avoid politics</td>\n",
       "      <td>New lineup: The newly elected Nahdlatul Ulama ...</td>\n",
       "      <td>http://www.thejakartapost.com/news/2015/08/07/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015</td>\n",
       "      <td>NU</td>\n",
       "      <td>True</td>\n",
       "      <td>[[New, lineup, :, The, newly, elected, Nahdlat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fri, April 1 2016</td>\n",
       "      <td>National scene: NU to hold intÍl forum for Mus...</td>\n",
       "      <td>The countryÍs largest Islamic organization, Na...</td>\n",
       "      <td>http://www.thejakartapost.com/news/2016/04/01/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016</td>\n",
       "      <td>NU</td>\n",
       "      <td>True</td>\n",
       "      <td>[[The, countryÍs, largest, Islamic, organizati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wed, August 9, 2017</td>\n",
       "      <td>PAN in no hurry to pick candidate for 2019 ele...</td>\n",
       "      <td>The National Mandate Party (PAN) says it will ...</td>\n",
       "      <td>http://www.thejakartapost.com/news/2017/08/09/...</td>\n",
       "      <td>PAN, Jokowi, Joko-Widodo, 2019-presidential-el...</td>\n",
       "      <td>2017</td>\n",
       "      <td>NU</td>\n",
       "      <td>True</td>\n",
       "      <td>[[The, National, Mandate, Party, (, PAN, ), sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Date                                              Title  \\\n",
       "4    Mon, August 14 2017             Tension eases as school policy relaxed   \n",
       "3  Wed, October 16, 2013        Ganjar Kurnia: The Sundanese culture keeper   \n",
       "2    Fri, August 7, 2015    Reelected NU chairman pledges to avoid politics   \n",
       "1      Fri, April 1 2016  National scene: NU to hold intÍl forum for Mus...   \n",
       "0    Wed, August 9, 2017  PAN in no hurry to pick candidate for 2019 ele...   \n",
       "\n",
       "                                             Article  \\\n",
       "4  The controversy surrounding the proposed five-...   \n",
       "3  Ganjar Kurnia. JP/Novia D. Rulistia As a Sunda...   \n",
       "2  New lineup: The newly elected Nahdlatul Ulama ...   \n",
       "1  The countryÍs largest Islamic organization, Na...   \n",
       "0  The National Mandate Party (PAN) says it will ...   \n",
       "\n",
       "                                                Link  \\\n",
       "4  http://www.thejakartapost.com/news/2017/08/14/...   \n",
       "3  http://www.thejakartapost.com/news/2013/10/16/...   \n",
       "2  http://www.thejakartapost.com/news/2015/08/07/...   \n",
       "1  http://www.thejakartapost.com/news/2016/04/01/...   \n",
       "0  http://www.thejakartapost.com/news/2017/08/09/...   \n",
       "\n",
       "                                              Topics  Year Topic  is_NU  \\\n",
       "4                                                NaN  2017    NU   True   \n",
       "3                                                NaN  2013    NU   True   \n",
       "2                                                NaN  2015    NU   True   \n",
       "1                                                NaN  2016    NU   True   \n",
       "0  PAN, Jokowi, Joko-Widodo, 2019-presidential-el...  2017    NU   True   \n",
       "\n",
       "                                           sentences  \n",
       "4  [[The, controversy, surrounding, the, proposed...  \n",
       "3  [[Ganjar, Kurnia, .], [JP/Novia, D., Rulistia,...  \n",
       "2  [[New, lineup, :, The, newly, elected, Nahdlat...  \n",
       "1  [[The, countryÍs, largest, Islamic, organizati...  \n",
       "0  [[The, National, Mandate, Party, (, PAN, ), sa...  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NURandom = newsNU.sample(n=10)\n",
    "NURandom['sentences'] = NURandom['Article'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "NURandom.index = range(len(NURandom) - 1, -1,-1) #Reindex to make things nice in the future\n",
    "NURandom[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NURandom['POS_sents'] = NURandom['sentences'].apply(lambda x: stanford.postTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9    [[(Who, WP), (is, VBZ), (supposedly, RB), (res...\n",
       "8    [[(Nahdlatul, NNP), (Ulama, NNP), ((, NNP), (N...\n",
       "7    [[(A, DT), (fatwa, NN), (issued, VBN), (by, IN...\n",
       "6    [[(Open, NNP), (debate, NN), (:, :), (Particip...\n",
       "5    [[(In, IN), (what, WP), (is, VBZ), (a, DT), (r...\n",
       "4    [[(The, DT), (controversy, NN), (surrounding, ...\n",
       "3    [[(Ganjar, NNP), (Kurnia, NNP), (., .)], [(JP/...\n",
       "2    [[(New, JJ), (lineup, NN), (:, :), (The, DT), ...\n",
       "1    [[(The, DT), (countryÍs, NNPS), (largest, JJS)...\n",
       "0    [[(The, DT), (National, NNP), (Mandate, NNP), ...\n",
       "Name: POS_sents, dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NURandom['POS_sents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>As we count the number of nouns (NN) it seems that 'organization' defines NU. NU's more organizational structure in comparison to FPI is reflected in other top nouns such as 'chairman', 'congress' and 'meeting', which suggests some formal gathering and NU's hierarchy.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(')', 20),\n",
       " ('organization', 15),\n",
       " ('î', 14),\n",
       " ('chairman', 13),\n",
       " ('radicalism', 12),\n",
       " ('congress', 12),\n",
       " ('sharia', 11),\n",
       " ('policy', 11),\n",
       " ('leader', 10),\n",
       " (\"'Û\\x9d\", 10),\n",
       " ('meeting', 9),\n",
       " ('culture', 9),\n",
       " ('management', 8),\n",
       " ('country', 8),\n",
       " ('election', 8),\n",
       " ('percent', 7),\n",
       " ('year', 7),\n",
       " ('law', 7),\n",
       " ('shopping', 7),\n",
       " ('center', 7)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'NN'\n",
    "targetCounts = {}\n",
    "for entry in NURandom['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>As we see in top verbs (VB) for NU, there are more words that imply cooperation rather than contention as we see in articles for FPI. This is apparent in words such as 'discuss', 'comply', 'help', 'promote' and 'ensure'.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 24),\n",
       " ('make', 10),\n",
       " ('prevent', 5),\n",
       " ('discuss', 5),\n",
       " ('have', 4),\n",
       " ('determine', 4),\n",
       " ('take', 4),\n",
       " ('get', 4),\n",
       " ('organize', 4),\n",
       " ('lead', 3),\n",
       " ('know', 3),\n",
       " ('comply', 3),\n",
       " ('help', 3),\n",
       " ('promote', 3),\n",
       " ('bring', 3),\n",
       " ('address', 2),\n",
       " ('mean', 2),\n",
       " ('ensure', 2),\n",
       " ('ask', 2),\n",
       " ('stay', 2)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'VB'\n",
    "targetCounts = {}\n",
    "for entry in NURandom['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i)Let's see what kind of adjective (JJ) modifies NU as an 'organization'. It's very interesting to compare FPI that is portrayed as a hard-line group and NU as an 'Islamic' / 'Muslim' organization as portrayed by the media.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Islamic', 'Muslim'}\n"
     ]
    }
   ],
   "source": [
    "NTarget = 'JJ'\n",
    "Word = 'organization'\n",
    "NResults = set()\n",
    "for entry in NURandom['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for (ent1, kind1),(ent2,kind2) in zip(sentence[:-1], sentence[1:]):\n",
    "            if (kind1,ent2.lower())==(NTarget,Word):\n",
    "                NResults.add(ent1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(NResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named-Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) is also a classification task, which identifies named objects. Included with Stanford NER are a 4 class model trained on the CoNLL 2003 eng.train, a 7 class model trained on the MUC 6 and MUC 7 training data sets, and a 3 class model trained on both data sets plus some additional data (including ACE 2002 and limited data in-house) on the intersection of those class sets. \n",
    "\n",
    "**3 class**:\tLocation, Person, Organization\n",
    "\n",
    "**4 class**:\tLocation, Person, Organization, Misc\n",
    "\n",
    "**7 class**:\tLocation, Person, Organization, Money, Percent, Date, Time\n",
    "\n",
    "These models each use distributional similarity features, which provide some performance gain at the cost of increasing their size and runtime. Also available are the same models missing those features.\n",
    "\n",
    "(We note that the training data for the 3 class model does not include any material from the CoNLL eng.testa or eng.testb data sets, nor any of the MUC 6 or 7 test or devtest datasets, nor Alan Ritter's Twitter NER data, so all of these would be valid tests of its performance.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we tag our first set of exemplary sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('I', 'O'), ('saw', 'O'), ('the', 'O'), ('elephant', 'O'), ('in', 'O'), ('my', 'O'), ('pajamas', 'O'), ('.', 'O')], [('The', 'O'), ('quick', 'O'), ('brown', 'O'), ('fox', 'O'), ('jumped', 'O'), ('over', 'O'), ('the', 'O'), ('lazy', 'O'), ('dog', 'O'), ('.', 'O')], [('While', 'O'), ('in', 'O'), ('France', 'LOCATION'), (',', 'O'), ('Christine', 'PERSON'), ('Lagarde', 'PERSON'), ('discussed', 'O'), ('short-term', 'O'), ('stimulus', 'O'), ('efforts', 'O'), ('in', 'O'), ('a', 'O'), ('recent', 'O'), ('interview', 'O'), ('with', 'O'), ('the', 'O'), ('Wall', 'ORGANIZATION'), ('Street', 'ORGANIZATION'), ('Journal', 'ORGANIZATION'), ('.', 'O')], [('Trayvon', 'PERSON'), ('Benjamin', 'PERSON'), ('Martin', 'PERSON'), ('was', 'O'), ('an', 'O'), ('African', 'O'), ('American', 'O'), ('from', 'O'), ('Miami', 'LOCATION'), ('Gardens', 'LOCATION'), (',', 'O'), ('Florida', 'LOCATION'), (',', 'O'), ('who', 'O'), (',', 'O'), ('at', 'O'), ('17', 'O'), ('years', 'O'), ('old', 'O'), (',', 'O'), ('was', 'O'), ('fatally', 'O'), ('shot', 'O'), ('by', 'O'), ('George', 'PERSON'), ('Zimmerman', 'PERSON'), (',', 'O'), ('a', 'O'), ('neighborhood', 'O'), ('watch', 'O'), ('volunteer', 'O'), (',', 'O'), ('in', 'O'), ('Sanford', 'LOCATION'), (',', 'O'), ('Florida', 'LOCATION'), ('.', 'O')], [('Buffalo', 'LOCATION'), ('buffalo', 'O'), ('Buffalo', 'ORGANIZATION'), ('buffalo', 'O'), ('buffalo', 'O'), ('buffalo', 'O'), ('Buffalo', 'ORGANIZATION'), ('buffalo', 'O')]]\n"
     ]
    }
   ],
   "source": [
    "classified_sents = stanford.nerTagger.tag_sents(tokenized_text)\n",
    "print(classified_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run NER over our entire corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redditTopScores['classified_sents'] = redditTopScores['sentences'].apply(lambda x: stanford.nerTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9    [[(Last, O), (year, O), (,, O), (Help, O), (De...\n",
       "8    [[(First, O), (post, O), (in, O), (quite, O), ...\n",
       "7    [[([, O), (Original, O), (Post, O), (], O), ((...\n",
       "6    [[(I, O), (witnessed, O), (this, O), (astoundi...\n",
       "5    [[(I, O), (work, O), (Helpdesk, ORGANIZATION),...\n",
       "4    [[(This, O), (just, O), (happened, O), (..., O...\n",
       "3    [[(Another, O), (tale, O), (from, O), (the, O)...\n",
       "2    [[([, O), (Part, O), (1, O), (], O), ((, O), (...\n",
       "1    [[(>, O), ($, O), (Me, O), (-, O), (Hello, O),...\n",
       "0    [[(So, O), (my, O), (story, O), (starts, O), (...\n",
       "Name: classified_sents, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redditTopScores['classified_sents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most common entities (which are, of course, boring):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 401),\n",
       " ('I', 245),\n",
       " ('the', 226),\n",
       " (',', 205),\n",
       " ('to', 197),\n",
       " ('a', 143),\n",
       " ('and', 135),\n",
       " ('>', 106),\n",
       " ('you', 102),\n",
       " ('of', 97)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entityCounts = {}\n",
    "for entry in redditTopScores['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if ent in entityCounts:\n",
    "                entityCounts[ent] += 1\n",
    "            else:\n",
    "                entityCounts[ent] = 1\n",
    "sortedEntities = sorted(entityCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedEntities[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or those occurring only twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year',\n",
       " 'Desk',\n",
       " 'busy',\n",
       " 'fix',\n",
       " 'received',\n",
       " 'couple',\n",
       " 'Windows',\n",
       " 'anymore',\n",
       " 'Sure',\n",
       " 'error',\n",
       " 'DVD',\n",
       " 'opened',\n",
       " 'There',\n",
       " 'upside',\n",
       " 'local',\n",
       " 'bane',\n",
       " 'existence',\n",
       " 'learn',\n",
       " 'sometimes',\n",
       " 'generic',\n",
       " 'Everyone',\n",
       " 'login',\n",
       " 'times',\n",
       " 'guy',\n",
       " 'asset',\n",
       " 'name',\n",
       " 'Computer',\n",
       " 'nothing',\n",
       " \"'P4ssword\",\n",
       " 'P',\n",
       " 'Everything',\n",
       " 'case',\n",
       " '*type',\n",
       " 'S',\n",
       " 'LOWERCASE',\n",
       " 'used',\n",
       " 'four',\n",
       " 'Original',\n",
       " 'cancer',\n",
       " 'month',\n",
       " 'live',\n",
       " 'brave',\n",
       " 'bitter',\n",
       " 'passed',\n",
       " 'ago',\n",
       " 'absolutely',\n",
       " 'ready',\n",
       " 'proud',\n",
       " 'above',\n",
       " 'completely',\n",
       " 'its',\n",
       " 'meant',\n",
       " 'both',\n",
       " 'sharing',\n",
       " 'making',\n",
       " '100',\n",
       " 'share',\n",
       " 'looking',\n",
       " 'ALL',\n",
       " 'whom',\n",
       " 'business',\n",
       " 'whose',\n",
       " 'stronger',\n",
       " 'bad',\n",
       " 'mess',\n",
       " 'turn',\n",
       " 'first',\n",
       " 'others',\n",
       " 'Here',\n",
       " 'suggested',\n",
       " 'videos',\n",
       " 'While',\n",
       " 'stand',\n",
       " 'certain',\n",
       " 'enjoy',\n",
       " 'well',\n",
       " 'drowned',\n",
       " 'soon',\n",
       " 'understand',\n",
       " 'risks',\n",
       " 'myself',\n",
       " 'point',\n",
       " 'future',\n",
       " 'avoid',\n",
       " 'thinking',\n",
       " 'information',\n",
       " 'insurance',\n",
       " 'site',\n",
       " 'step',\n",
       " 'guide',\n",
       " 'discover',\n",
       " 'order',\n",
       " '5',\n",
       " 'slightly',\n",
       " 'spent',\n",
       " 'moment',\n",
       " 'arms',\n",
       " 'idea',\n",
       " 'food',\n",
       " 'party',\n",
       " 'played',\n",
       " 'family',\n",
       " 'allowed',\n",
       " 'cry',\n",
       " 'pretty',\n",
       " 'nice',\n",
       " 'loved',\n",
       " 'mind',\n",
       " 'favor',\n",
       " 'watched',\n",
       " 'Things',\n",
       " '17',\n",
       " 'small',\n",
       " 'lived',\n",
       " 'living',\n",
       " 'themselves',\n",
       " 'potential',\n",
       " 'happiness',\n",
       " 'sound',\n",
       " 'situation',\n",
       " 'believe',\n",
       " 'mistakes',\n",
       " 'same',\n",
       " 'scenario',\n",
       " 'difference',\n",
       " 'glad',\n",
       " 'flaws',\n",
       " 'stupid',\n",
       " 'yourself',\n",
       " 'ok',\n",
       " 'In',\n",
       " 'comments',\n",
       " 'SO',\n",
       " 'random',\n",
       " 'request',\n",
       " 'Give',\n",
       " 'THIS',\n",
       " 'response',\n",
       " 'Thanks',\n",
       " 'tears',\n",
       " 'helped',\n",
       " 'reply',\n",
       " 'large',\n",
       " 'academic',\n",
       " 'organization',\n",
       " 'CEO',\n",
       " 'Of',\n",
       " 'course',\n",
       " 'mailboxes',\n",
       " 'Fail',\n",
       " '#',\n",
       " 'check',\n",
       " 'generate',\n",
       " '=',\n",
       " 'real',\n",
       " 'stopped',\n",
       " 'avalanche',\n",
       " 'died',\n",
       " 'systems',\n",
       " 'staff',\n",
       " 'brought',\n",
       " 'retail',\n",
       " 'store',\n",
       " 'plugged',\n",
       " 'hear',\n",
       " 'occasionally',\n",
       " 'operate',\n",
       " 'drawer*',\n",
       " 'try',\n",
       " 'hit',\n",
       " '*I',\n",
       " 'echo',\n",
       " 'heard',\n",
       " 'seconds',\n",
       " 'nose',\n",
       " 'working',\n",
       " 'BING',\n",
       " 'THE',\n",
       " '*Note',\n",
       " 'yes',\n",
       " 'different',\n",
       " 'search',\n",
       " 'immediately',\n",
       " 'connection',\n",
       " 'Turns',\n",
       " 'shortcut',\n",
       " 'okay',\n",
       " 'computering',\n",
       " 'taking',\n",
       " 'Steve',\n",
       " 'XYZ',\n",
       " 'however',\n",
       " 'forwarded',\n",
       " 'using',\n",
       " 'meet',\n",
       " 'ran',\n",
       " 'mouse',\n",
       " 'closed',\n",
       " 'window',\n",
       " 'revealing',\n",
       " 'me*',\n",
       " 'shaking',\n",
       " 'lunch',\n",
       " 'yelled',\n",
       " 'needed',\n",
       " 'key',\n",
       " 'week',\n",
       " 'pointed',\n",
       " 'door',\n",
       " 'blinked',\n",
       " 'Then',\n",
       " 'command',\n",
       " 'three',\n",
       " 'web',\n",
       " 'pages',\n",
       " 'person',\n",
       " 'Whatever',\n",
       " 'um',\n",
       " 'wrong',\n",
       " 'mother',\n",
       " 'done',\n",
       " 'way',\n",
       " 'weeks',\n",
       " 'since',\n",
       " 'stuff',\n",
       " 'took',\n",
       " 'Wow',\n",
       " 'gildings',\n",
       " 'One',\n",
       " 'HR',\n",
       " 'select',\n",
       " '*Are',\n",
       " 'Ca',\n",
       " 'building',\n",
       " 'holiday',\n",
       " 'pay',\n",
       " 'gods',\n",
       " 'expire',\n",
       " '60',\n",
       " 'anyway',\n",
       " 'User',\n",
       " 'DeskMugPhonePencil1',\n",
       " 'Thursday',\n",
       " 'return',\n",
       " 'calls',\n",
       " 'often',\n",
       " 'issues',\n",
       " 'service',\n",
       " 'older',\n",
       " 'types',\n",
       " 'speak',\n",
       " 'box',\n",
       " 'properly',\n",
       " 'personally',\n",
       " 'ask',\n",
       " 'supervisor',\n",
       " 'earlier',\n",
       " 'visit',\n",
       " 'terrible',\n",
       " 'willing',\n",
       " 'nasty']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0] for x in sortedEntities if x[1] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also list the most common \"non-objects\". (We note that we're not graphing these because there are so few here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jack', 17),\n",
       " ('Google', 6),\n",
       " ('Smith', 5),\n",
       " ('Steve', 2),\n",
       " ('Citrix', 1),\n",
       " ('Nono', 1),\n",
       " ('Reddit', 1),\n",
       " ('Helpdesk', 1),\n",
       " ('UK', 1),\n",
       " ('CMD', 1)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonObjCounts = {}\n",
    "for entry in redditTopScores['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind == 'O':\n",
    "                continue\n",
    "            elif ent in nonObjCounts:\n",
    "                nonObjCounts[ent] += 1\n",
    "            else:\n",
    "                nonObjCounts[ent] = 1\n",
    "sortedNonObj = sorted(nonObjCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedNonObj[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the Organizations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Google', 6), ('Citrix', 1), ('Helpdesk', 1), ('CMD', 1), ('GOOGLE', 1)]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OrgCounts = {}\n",
    "for entry in redditTopScores['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != 'ORGANIZATION':\n",
    "                continue\n",
    "            elif ent in OrgCounts:\n",
    "                OrgCounts[ent] += 1\n",
    "            else:\n",
    "                OrgCounts[ent] = 1\n",
    "sortedOrgs = sorted(OrgCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedOrgs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These, of course, have much smaller counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 2*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform NER on a (modest) subset of your corpus of interest. List all of the different kinds of entities tagged? What does their distribution suggest about the focus of your corpus? For a subset of your corpus, tally at least one type of named entity and calculate the Precision, Recall and F-score for the NER classification just performed (using your own hand-codings as \"ground truth\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Islamic Defender Front</h3>\n",
    "\n",
    "Let's run NER for an article on FPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Article</th>\n",
       "      <th>Link</th>\n",
       "      <th>Topics</th>\n",
       "      <th>Year</th>\n",
       "      <th>Topic</th>\n",
       "      <th>is_NU</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sat, November 19, 2016</td>\n",
       "      <td>Jokowi should meet with FPI to tone down prote...</td>\n",
       "      <td>House of Representatives Deputy Speaker Fahri ...</td>\n",
       "      <td>http://www.thejakartapost.com/news/2016/11/19/...</td>\n",
       "      <td>Habib-Rizieq-Shihab, FPI, ahok, blasphemy, def...</td>\n",
       "      <td>2016</td>\n",
       "      <td>FPI</td>\n",
       "      <td>False</td>\n",
       "      <td>[[House, of, Representatives, Deputy, Speaker,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Date                                              Title  \\\n",
       "0  Sat, November 19, 2016  Jokowi should meet with FPI to tone down prote...   \n",
       "\n",
       "                                             Article  \\\n",
       "0  House of Representatives Deputy Speaker Fahri ...   \n",
       "\n",
       "                                                Link  \\\n",
       "0  http://www.thejakartapost.com/news/2016/11/19/...   \n",
       "\n",
       "                                              Topics  Year Topic  is_NU  \\\n",
       "0  Habib-Rizieq-Shihab, FPI, ahok, blasphemy, def...  2016   FPI  False   \n",
       "\n",
       "                                           sentences  \n",
       "0  [[House, of, Representatives, Deputy, Speaker,...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FPIRandom2 = newsFPI.sample(n=1)\n",
    "FPIRandom2['sentences'] = FPIRandom2['Article'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "FPIRandom2.index = range(len(FPIRandom2) - 1, -1,-1) #Reindex to make things nice in the future\n",
    "FPIRandom2[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Classify news articles\n",
    "\n",
    "FPIRandom2['classified_sents'] = FPIRandom2['sentences'].apply(lambda x: stanford.nerTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[(House, O), (of, O), (Representatives, O), (...\n",
       "Name: classified_sents, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FPIRandom2['classified_sents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most common entities:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could list the most common \"non-objects\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jokowi', 7),\n",
       " ('Pak', 3),\n",
       " ('Prabowo', 3),\n",
       " ('Fahri', 2),\n",
       " ('Rizieq', 2),\n",
       " ('Party', 2),\n",
       " ('Hamzah', 1),\n",
       " ('Joko', 1),\n",
       " ('ñJokowiî', 1),\n",
       " ('Widodo', 1)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonObjCounts = {}\n",
    "for entry in FPIRandom2['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind == 'O':\n",
    "                continue\n",
    "            elif ent in nonObjCounts:\n",
    "                nonObjCounts[ent] += 1\n",
    "            else:\n",
    "                nonObjCounts[ent] = 1\n",
    "sortedNonObj = sorted(nonObjCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedNonObj[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at organizations, location and person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Party', 2),\n",
       " ('Islam', 1),\n",
       " ('Defenders', 1),\n",
       " ('Front', 1),\n",
       " ('FPI', 1),\n",
       " ('Gerindra', 1),\n",
       " ('Indonesian', 1),\n",
       " ('Military', 1),\n",
       " ('National', 1),\n",
       " ('Police', 1)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OrgCounts = {}\n",
    "for entry in FPIRandom2['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != 'ORGANIZATION':\n",
    "                continue\n",
    "            elif ent in OrgCounts:\n",
    "                OrgCounts[ent] += 1\n",
    "            else:\n",
    "                OrgCounts[ent] = 1\n",
    "sortedOrgs = sorted(OrgCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedOrgs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jokowi', 6),\n",
       " ('Pak', 3),\n",
       " ('Prabowo', 3),\n",
       " ('Fahri', 2),\n",
       " ('Rizieq', 2),\n",
       " ('Hamzah', 1),\n",
       " ('Joko', 1),\n",
       " ('ñJokowiî', 1),\n",
       " ('Widodo', 1),\n",
       " ('Habib', 1)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OrgCounts = {}\n",
    "for entry in FPIRandom2['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != 'PERSON':\n",
    "                continue\n",
    "            elif ent in OrgCounts:\n",
    "                OrgCounts[ent] += 1\n",
    "            else:\n",
    "                OrgCounts[ent] = 1\n",
    "sortedOrgs = sorted(OrgCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedOrgs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jokowi', 1),\n",
       " ('Presidential', 1),\n",
       " ('Palace', 1),\n",
       " ('Jakarta', 1),\n",
       " ('Tjahaja', 1),\n",
       " ('Purnama', 1)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OrgCounts = {}\n",
    "for entry in FPIRandom2['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != 'LOCATION':\n",
    "                continue\n",
    "            elif ent in OrgCounts:\n",
    "                OrgCounts[ent] += 1\n",
    "            else:\n",
    "                OrgCounts[ent] = 1\n",
    "sortedOrgs = sorted(OrgCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedOrgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our text to establish the ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House of Representatives Deputy Speaker Fahri Hamzah has suggested President Joko ñJokowiî Widodo meet leaders of Islamic organizations, including Habib Rizieq of the Islam Defenders Front (FPI) who previously arranged a Nov. 4 mass demonstration, to easeætensions. ñIf Pak Jokowi can be friends with Pak Prabowo, why not with them [FPI]?î said the controversial lawmaker on Friday, referring to Gerindra Party chairperson Prabowo Subianto, who lost the 2014 presidential race to Jokowi. Jokowi has recently met with Prabowo twice, with the latest meeting taking place at the Presidential Palace on Thursday. This has been seen as a move by Jokowi to secure PrabowoÍs support amid mounting protests against his presidency over allegations the President has not been impartial in his response to the blasphemy case impacting non-active Jakarta Governor Basuki ñAhokî Tjahaja Purnama. Jokowi has also conducted a series of visits to state security institutions, including the Indonesian Military and National Police. ñI donÍt understand why Pak Jokowi doesnÍt think itÍs important to meet leaders of the mass demonstration, whom he refused to meet on Nov. 4 [] but agreed to meet leaders of various other groups?î the Islamic-based Prosperous Justice Party (PKS) politician added. Fahri was confident that if Jokowi met with Rizieq and his supporters, criticism against his administration could be eased, particularly ahead of another planned demonstration slated for Dec. 2. (ebf)\n"
     ]
    }
   ],
   "source": [
    "print(FPIRandom2['Article'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>From a manual hand-coding I have established these classifications:</i>\n",
    "    \n",
    "ORGANIZATION:\n",
    "    Islamic (2);\n",
    "    Islam (1);\n",
    "    Defenders (1);\n",
    "    Front (1);\n",
    "    FPI (2);\n",
    "    Gerindra (1);\n",
    "    Party (2);\n",
    "    State (1);\n",
    "    Security (1);\n",
    "    Indonesian (1);\n",
    "    Military (1);\n",
    "    National (1);\n",
    "    Police (1);\n",
    "    Prosperousn (1);\n",
    "    Justice (1);\n",
    "    PKS (1).\n",
    "\n",
    "PERSON:\n",
    "    Deputy (1);\n",
    "    Speaker (1);\n",
    "    Fahri (2);\n",
    "    Hamzah (1);\n",
    "    President (2);\n",
    "    Joko (1);\n",
    "    Widodo (1);\n",
    "    Habib (1);\n",
    "    Rizieq (2);\n",
    "    Pak (3);\n",
    "    Jokowi (8);\n",
    "    Prabowo (4);\n",
    "    Subianto (1);\n",
    "    chairperson (1);\n",
    "    Governor (1);\n",
    "    Basuki (1);\n",
    "    Tjahaja (1);\n",
    "    Purnama (1);\n",
    "    Ahok (1).\n",
    "\n",
    "LOCATION:\n",
    "    Jakarta (1);\n",
    "    state (1);\n",
    "    security (1);\n",
    "    institutions (1);\n",
    "    Presidential (1);\n",
    "    Palace (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision**\n",
    "\n",
    "*Let's look at location's precision (false positives).*\n",
    "\n",
    "    Precision = classified right / classified right and wrong\n",
    "              = 3 / 6\n",
    "              = 0.5\n",
    "              \n",
    "*The precision is only <b>0.5</b>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall**\n",
    "\n",
    "*Let's look at location's recall.*\n",
    "\n",
    "    Precision = classified right / total right possible\n",
    "              = 3 / 6\n",
    "              = 0.5\n",
    "              \n",
    "*The recall is only <b>0.5</b>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1-Measure**\n",
    "\n",
    "*Let's look at location's F1-Measure.*\n",
    "\n",
    "    Precision = 2 * ((precision / recall) / (precision + recall)\n",
    "              = 2 * (0.25 / 1)\n",
    "              = 2 * 0.25\n",
    "              = 0.5\n",
    "              \n",
    "*The FI-eeasure is only <b>0.5</b>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing\n",
    "\n",
    "Here we will introduce the Stanford Parser by feeding it tokenized text from our initial example sentences. The parser is a dependency parser, but this initial program outputs a simple, self-explanatory phrase-structure representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('NNP', ['Trayvon']), Tree('NNP', ['Benjamin']), Tree('NNP', ['Martin'])]), Tree('VP', [Tree('VBD', ['was']), Tree('NP', [Tree('NP', [Tree('DT', ['an']), Tree('NNP', ['African']), Tree('NNP', ['American'])]), Tree('PP', [Tree('IN', ['from']), Tree('NP', [Tree('NP', [Tree('NNP', ['Miami']), Tree('NNPS', ['Gardens'])]), Tree(',', [',']), Tree('NP', [Tree('NNP', ['Florida'])]), Tree(',', [',']), Tree('SBAR', [Tree('WHNP', [Tree('WP', ['who'])]), Tree('S', [Tree(',', [',']), Tree('PP', [Tree('IN', ['at']), Tree('ADJP', [Tree('NP', [Tree('CD', ['17']), Tree('NNS', ['years'])]), Tree('JJ', ['old'])])]), Tree(',', [',']), Tree('VP', [Tree('VBD', ['was']), Tree('ADVP', [Tree('RB', ['fatally'])]), Tree('VP', [Tree('VBN', ['shot']), Tree('PP', [Tree('IN', ['by']), Tree('NP', [Tree('NP', [Tree('NNP', ['George']), Tree('NNP', ['Zimmerman'])]), Tree(',', [',']), Tree('NP', [Tree('DT', ['a']), Tree('NN', ['neighborhood']), Tree('NN', ['watch']), Tree('NN', ['volunteer'])]), Tree(',', [','])])]), Tree('PP', [Tree('IN', ['in']), Tree('NP', [Tree('NNP', ['Sanford']), Tree(',', [',']), Tree('NNP', ['Florida'])])])])])])])])])])]), Tree('.', ['.'])])])]\n"
     ]
    }
   ],
   "source": [
    "parses = list(stanford.parser.parse_sents(tokenized_text)) #Converting the iterator to a list so we can call by index. They are still \n",
    "fourthSentParseTree = list(parses[3]) #iterators so be careful about re-running code, without re-running this block\n",
    "print(fourthSentParseTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trees are a common data structure and there are a large number of things to do with them. What we are intetered in is the relationship between different types of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def treeRelation(parsetree, relationType, *targets):\n",
    "    if isinstance(parsetree, list):\n",
    "        parsetree = parsetree[0]\n",
    "    if set(targets) & set(parsetree.leaves()) != set(targets):\n",
    "        return []\n",
    "    else:\n",
    "        retList = []\n",
    "        for subT in parsetree.subtrees():\n",
    "            if subT.label() == relationType:\n",
    "                if set(targets) & set(subT.leaves()) == set(targets):\n",
    "                    retList.append([(subT.label(), ' '.join(subT.leaves()))])\n",
    "    return retList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def treeSubRelation(parsetree, relationTypeScope, relationTypeTarget, *targets):\n",
    "    if isinstance(parsetree, list):\n",
    "        parsetree = parsetree[0]\n",
    "    if set(targets) & set(parsetree.leaves()) != set(targets):\n",
    "        return []\n",
    "    else:\n",
    "        retSet = set()\n",
    "        for subT in parsetree.subtrees():\n",
    "            if set(targets) & set(subT.leaves()) == set(targets):\n",
    "                if subT.label() == relationTypeScope:\n",
    "                    for subsub in subT.subtrees():\n",
    "                        if subsub.label()==relationTypeTarget:\n",
    "                            retSet.add(' '.join(subsub.leaves()))\n",
    "    return retSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('NP',\n",
       "   'an African American from Miami Gardens , Florida , who , at 17 years old , was fatally shot by George Zimmerman , a neighborhood watch volunteer , in Sanford , Florida')],\n",
       " [('NP',\n",
       "   'Miami Gardens , Florida , who , at 17 years old , was fatally shot by George Zimmerman , a neighborhood watch volunteer , in Sanford , Florida')]]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeRelation(fourthSentParseTree, 'NP', 'Florida', 'who')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that Florida occurs twice in two different nested noun phrases in the sentence. \n",
    "\n",
    "We can also find all of the verbs within the noun phrase defined by one or more target words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shot'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeSubRelation(fourthSentParseTree, 'NP', 'VBN', 'Florida', 'who')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if we want to to look at the whole tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                   ROOT                                                                                                                       \n",
      "                                                                                                                    |                                                                                                                          \n",
      "                                                                                                                    S                                                                                                                         \n",
      "            ________________________________________________________________________________________________________|_______________________________________________________________________________________________________________________   \n",
      "           |                       VP                                                                                                                                                                                                       | \n",
      "           |              _________|______________                                                                                                                                                                                          |  \n",
      "           |             |                        NP                                                                                                                                                                                        | \n",
      "           |             |          ______________|________________                                                                                                                                                                         |  \n",
      "           |             |         |                               PP                                                                                                                                                                       | \n",
      "           |             |         |               ________________|___________                                                                                                                                                             |  \n",
      "           |             |         |              |                            NP                                                                                                                                                           | \n",
      "           |             |         |              |           _________________|____________________________________                                                                                                                        |  \n",
      "           |             |         |              |          |           |     |     |                             SBAR                                                                                                                     | \n",
      "           |             |         |              |          |           |     |     |    __________________________|______________________________                                                                                         |  \n",
      "           |             |         |              |          |           |     |     |   |                                                         S                                                                                        | \n",
      "           |             |         |              |          |           |     |     |   |     ____________________________________________________|_______________________                                                                 |  \n",
      "           |             |         |              |          |           |     |     |   |    |           |              |                                                 VP                                                               | \n",
      "           |             |         |              |          |           |     |     |   |    |           |              |    _____________________________________________|_______                                                         |  \n",
      "           |             |         |              |          |           |     |     |   |    |           |              |   |     |                                               VP                                                       | \n",
      "           |             |         |              |          |           |     |     |   |    |           |              |   |     |      _________________________________________|________________________________________                |  \n",
      "           |             |         |              |          |           |     |     |   |    |           PP             |   |     |     |                      PP                                                          |               | \n",
      "           |             |         |              |          |           |     |     |   |    |    _______|____          |   |     |     |     _________________|__________                                                 |               |  \n",
      "           |             |         |              |          |           |     |     |   |    |   |           ADJP       |   |     |     |    |                            NP                                               PP              | \n",
      "           |             |         |              |          |           |     |     |   |    |   |        ____|____     |   |     |     |    |           _________________|________________________________     ___________|___            |  \n",
      "           NP            |         NP             |          NP          |     NP    |  WHNP  |   |       NP        |    |   |    ADVP   |    |          NP            |           NP                       |   |               NP          | \n",
      "    _______|_______      |    _____|_______       |      ____|_____      |     |     |   |    |   |    ___|____     |    |   |     |     |    |     _____|______       |    _______|_________________       |   |      _________|_____      |  \n",
      "  NNP     NNP     NNP   VBD  DT   NNP     NNP     IN   NNP        NNPS   ,    NNP    ,   WP   ,   IN  CD      NNS   JJ   ,  VBD    RB   VBN   IN  NNP          NNP     ,   DT      NN        NN      NN     ,   IN   NNP        ,    NNP    . \n",
      "   |       |       |     |   |     |       |      |     |          |     |     |     |   |    |   |   |        |    |    |   |     |     |    |    |            |      |   |       |         |       |      |   |     |         |     |     |  \n",
      "Trayvon Benjamin Martin was  an African American from Miami     Gardens  ,  Florida  ,  who   ,   at  17     years old   ,  was fatally shot  by George     Zimmerman  ,   a  neighborhood watch volunteer  ,   in Sanford      ,  Florida  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fourthSentParseTree[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or another sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ROOT                           \n",
      "                      |                              \n",
      "                      S                             \n",
      "       _______________|___________________________   \n",
      "      |                          VP               | \n",
      "      |                __________|___             |  \n",
      "      |               |              PP           | \n",
      "      |               |      ________|___         |  \n",
      "      NP              |     |            NP       | \n",
      "  ____|__________     |     |     _______|____    |  \n",
      " DT   JJ    JJ   NN  VBD    IN   DT      JJ   NN  . \n",
      " |    |     |    |    |     |    |       |    |   |  \n",
      "The quick brown fox jumped over the     lazy dog  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "list(parses[1])[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency parsing and graph representations\n",
    "\n",
    "Dependency parsing was developed to robustly capture linguistic dependencies from text. The complex tags associated with these parses are detailed [here]('http://universaldependencies.org/u/overview/syntax.html'). When parsing with the dependency parser, we will work directly from the untokenized text. Note that no *processing* takes place before parsing sentences--we do not remove so-called stop words or anything that plays a syntactic role in the sentence, although anaphora resolution and related normalization may be performed before or after parsing to enhance the value of information extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function DependencyGraph.__init__.<locals>.<lambda> at 0x11024fd08>,\n",
      "            {0: {'address': 0,\n",
      "                 'ctag': 'TOP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'root': [5]}),\n",
      "                 'feats': None,\n",
      "                 'head': None,\n",
      "                 'lemma': None,\n",
      "                 'rel': None,\n",
      "                 'tag': 'TOP',\n",
      "                 'word': None},\n",
      "             1: {'address': 1,\n",
      "                 'ctag': 'DT',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'det',\n",
      "                 'tag': 'DT',\n",
      "                 'word': 'The'},\n",
      "             2: {'address': 2,\n",
      "                 'ctag': 'JJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'amod',\n",
      "                 'tag': 'JJ',\n",
      "                 'word': 'quick'},\n",
      "             3: {'address': 3,\n",
      "                 'ctag': 'JJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'amod',\n",
      "                 'tag': 'JJ',\n",
      "                 'word': 'brown'},\n",
      "             4: {'address': 4,\n",
      "                 'ctag': 'NN',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'amod': [2, 3],\n",
      "                                      'det': [1]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 5,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'nsubj',\n",
      "                 'tag': 'NN',\n",
      "                 'word': 'fox'},\n",
      "             5: {'address': 5,\n",
      "                 'ctag': 'VBD',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'nmod': [9],\n",
      "                                      'nsubj': [4]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 0,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'root',\n",
      "                 'tag': 'VBD',\n",
      "                 'word': 'jumped'},\n",
      "             6: {'address': 6,\n",
      "                 'ctag': 'IN',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 9,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'case',\n",
      "                 'tag': 'IN',\n",
      "                 'word': 'over'},\n",
      "             7: {'address': 7,\n",
      "                 'ctag': 'DT',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 9,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'det',\n",
      "                 'tag': 'DT',\n",
      "                 'word': 'the'},\n",
      "             8: {'address': 8,\n",
      "                 'ctag': 'JJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 9,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'amod',\n",
      "                 'tag': 'JJ',\n",
      "                 'word': 'lazy'},\n",
      "             9: {'address': 9,\n",
      "                 'ctag': 'NN',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'amod': [8],\n",
      "                                      'case': [6],\n",
      "                                      'det': [7]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 5,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'nmod',\n",
      "                 'tag': 'NN',\n",
      "                 'word': 'dog'}})\n"
     ]
    }
   ],
   "source": [
    "depParses = list(stanford.depParser.raw_parse_sents(text)) #Converting the iterator to a list so we can call by index. They are still \n",
    "secondSentDepParseTree = list(depParses[1])[0] #iterators so be careful about re-running code, without re-running this block\n",
    "print(secondSentDepParseTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a graph and we can convert it to a dot file and use that to visulize it. Try traversing the tree and extracting elements that are nearby one another. We note that unless you have the graphviz successfully installed on your computer (which is not necessary to complete this homework), the following graphviz call will trigger an error. If you are interested in installing graphviz and working on a Mac, consider installing through [homebrew](https://brew.sh), a package manager (i.e., with the command \"brew install graphviz\", once brew is installed). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"467pt\" height=\"302pt\"\n",
       " viewBox=\"0.00 0.00 467.37 302.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 298)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-298 463.3657,-298 463.3657,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"193.7949\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"193.7949\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">5 (jumped)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;5 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M193.7949,-257.7616C193.7949,-246.3597 193.7949,-231.4342 193.7949,-218.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"197.295,-218.2121 193.7949,-208.2121 190.295,-218.2121 197.295,-218.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"205.0708\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"152.7949\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (fox)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;4 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>5&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M184.9506,-171.6975C182.2192,-166.0286 179.2067,-159.7594 176.457,-154 172.9867,-146.731 169.2503,-138.8578 165.7948,-131.5568\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"168.7462,-129.6107 161.3081,-122.0658 162.4177,-132.6024 168.7462,-129.6107\"/>\n",
       "<text text-anchor=\"middle\" x=\"191.9639\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"316.7949\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">9 (dog)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;9 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M219.5798,-171.9716C237.9481,-159.1286 262.8214,-141.7376 282.8098,-127.7619\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"284.8541,-130.6033 291.044,-122.0047 280.843,-124.8665 284.8541,-130.6033\"/>\n",
       "<text text-anchor=\"middle\" x=\"278.7397\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"28.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (The)</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"108.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (quick)</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"195.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3 (brown)</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>4&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M126.8005,-85.9716C108.2827,-73.1286 83.2073,-55.7376 63.0563,-41.7619\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"64.967,-38.8277 54.7552,-36.0047 60.9777,-44.5797 64.967,-38.8277\"/>\n",
       "<text text-anchor=\"middle\" x=\"107.3467\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>4&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M143.4637,-85.7616C137.4551,-74.0176 129.534,-58.5355 122.7802,-45.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"125.7834,-43.5204 118.1127,-36.2121 119.5517,-46.7088 125.7834,-43.5204\"/>\n",
       "<text text-anchor=\"middle\" x=\"149.3467\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;3 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>4&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M161.9141,-85.7616C167.7861,-74.0176 175.5272,-58.5355 182.1275,-45.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"185.3472,-46.7216 186.6889,-36.2121 179.0862,-43.5911 185.3472,-46.7216\"/>\n",
       "<text text-anchor=\"middle\" x=\"193.3467\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"279.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">6 (over)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;6 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>9&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M308.9482,-85.7616C303.9446,-74.1316 297.3638,-58.8357 291.721,-45.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"294.7976,-44.0148 287.6304,-36.2121 288.3674,-46.7813 294.7976,-44.0148\"/>\n",
       "<text text-anchor=\"middle\" x=\"312.8398\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"354.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">7 (the)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;7 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>9&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M324.8537,-85.7616C329.9926,-74.1316 336.7512,-58.8357 342.5466,-45.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"345.9074,-46.7736 346.7477,-36.2121 339.5046,-43.9444 345.9074,-46.7736\"/>\n",
       "<text text-anchor=\"middle\" x=\"347.3467\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"429.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">8 (lazy)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;8 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>9&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M340.4834,-85.9716C357.2078,-73.2433 379.8019,-56.0478 398.0799,-42.1371\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"400.2997,-44.8461 406.1376,-36.0047 396.0604,-39.2758 400.2997,-44.8461\"/>\n",
       "<text text-anchor=\"middle\" x=\"396.3467\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x110397c18>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    secondSentGraph = graphviz.Source(secondSentDepParseTree.to_dot())\n",
    "except:\n",
    "    secondSentGraph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "secondSentGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or another sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"997pt\" height=\"560pt\"\n",
       " viewBox=\"0.00 0.00 996.96 560.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 556)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-556 992.9575,-556 992.9575,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"235.3833\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"235.3833\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">7 (American)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;7 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M235.3833,-515.7616C235.3833,-504.3597 235.3833,-489.4342 235.3833,-476.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"238.8834,-476.2121 235.3833,-466.2121 231.8834,-476.2121 238.8834,-476.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"246.6592\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"77.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3 (Martin)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;3 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>7&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M195.5259,-429.8504C183.9641,-424.3453 171.399,-418.1273 160.0454,-412 144.9107,-403.8322 128.6384,-394.1858 114.5678,-385.5556\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"116.0198,-382.3384 105.6735,-380.0569 112.3388,-388.2925 116.0198,-382.3384\"/>\n",
       "<text text-anchor=\"middle\" x=\"175.5522\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"161.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (was)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;4 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>7&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M219.6898,-429.7616C209.1921,-417.5615 195.2231,-401.3273 183.5899,-387.8076\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"186.2297,-385.5094 177.0542,-380.2121 180.9236,-390.0751 186.2297,-385.5094\"/>\n",
       "<text text-anchor=\"middle\" x=\"213.4902\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cop</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"235.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">5 (an)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;5 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>7&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M235.3833,-429.7616C235.3833,-418.3597 235.3833,-403.4342 235.3833,-390.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"238.8834,-390.2121 235.3833,-380.2121 231.8834,-390.2121 238.8834,-390.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"243.9351\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"319.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">6 (African)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;6 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>7&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M253.1975,-429.7616C265.2253,-417.4475 281.2673,-401.0235 294.5454,-387.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"297.1111,-389.8115 301.5947,-380.2121 292.1035,-384.9203 297.1111,-389.8115\"/>\n",
       "<text text-anchor=\"middle\" x=\"312.9351\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"421.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">10 (Gardens)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;10 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M281.035,-436.267C301.523,-430.2297 325.6866,-422.0184 346.3833,-412 361.1468,-404.8536 376.3934,-395.1776 389.2526,-386.2414\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"391.6132,-388.8578 397.7385,-380.213 387.5592,-383.1511 391.6132,-388.8578\"/>\n",
       "<text text-anchor=\"middle\" x=\"386.3281\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"41.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (Trayvon)</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"146.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (Benjamin)</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>3&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M69.7486,-343.7616C64.8803,-332.1316 58.4773,-316.8357 52.987,-303.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"56.097,-302.085 49.007,-294.2121 49.6399,-304.788 56.097,-302.085\"/>\n",
       "<text text-anchor=\"middle\" x=\"91.9351\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>3&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M105.7905,-343.9005C112.4839,-338.7035 119.1771,-332.628 124.3833,-326 129.5733,-319.3927 133.8059,-311.3704 137.1222,-303.7037\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"140.4118,-304.9025 140.8359,-294.3161 133.9027,-302.3275 140.4118,-304.9025\"/>\n",
       "<text text-anchor=\"middle\" x=\"162.9351\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"274.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">8 (from)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;8 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>10&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M376.5212,-344.6694C364.1537,-339.2688 350.9417,-332.9006 339.2935,-326 326.8103,-318.6047 313.97,-309.165 303.0389,-300.4839\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"305.0617,-297.6182 295.0895,-294.0398 300.6536,-303.0559 305.0617,-297.6182\"/>\n",
       "<text text-anchor=\"middle\" x=\"351.4282\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"360.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">9 (Miami)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;9 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>10&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M391.2568,-343.6347C384.9071,-338.5898 378.7774,-332.6501 374.2798,-326 369.9075,-319.5352 366.903,-311.7204 364.8413,-304.2096\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"368.2186,-303.2775 362.5744,-294.3112 361.3952,-304.8402 368.2186,-303.2775\"/>\n",
       "<text text-anchor=\"middle\" x=\"403.9351\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"456.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">12 (Florida)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;12 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>10&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M428.8059,-343.7616C433.539,-332.1316 439.7641,-316.8357 445.1019,-303.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"448.4436,-304.7938 448.9714,-294.2121 441.96,-302.1551 448.4436,-304.7938\"/>\n",
       "<text text-anchor=\"middle\" x=\"457.7144\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">appos</text>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>23</title>\n",
       "<text text-anchor=\"middle\" x=\"549.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">23 (shot)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;23 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>10&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M450.562,-343.9652C459.5457,-338.3073 469.4264,-331.9769 478.3833,-326 490.7955,-317.7174 504.223,-308.3654 516.0254,-300.0089\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"518.1726,-302.7768 524.2944,-294.1295 514.1163,-297.0718 518.1726,-302.7768\"/>\n",
       "<text text-anchor=\"middle\" x=\"520.9214\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">acl:relcl</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>14</title>\n",
       "<text text-anchor=\"middle\" x=\"341.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">14 (who)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;14 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>23&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M516.1602,-260.9769C513.2216,-259.8878 510.2679,-258.8756 507.3833,-258 465.6572,-245.3346 451.0323,-257.1374 410.9351,-240 396.1578,-233.6843 381.463,-223.7913 369.4142,-214.4665\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"371.5209,-211.6696 361.5269,-208.1526 367.1463,-217.1344 371.5209,-211.6696\"/>\n",
       "<text text-anchor=\"middle\" x=\"438.6074\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubjpass</text>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>19</title>\n",
       "<text text-anchor=\"middle\" x=\"423.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">19 (old)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;19 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>23&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M516.1838,-257.9822C506.6151,-252.48 496.2767,-246.2305 487.0659,-240 475.5821,-232.2319 463.494,-223.0275 452.972,-214.6466\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"454.892,-211.6986 444.9108,-208.1451 450.4974,-217.1473 454.892,-211.6986\"/>\n",
       "<text text-anchor=\"middle\" x=\"502.542\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advcl</text>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>21</title>\n",
       "<text text-anchor=\"middle\" x=\"504.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">21 (was)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;21 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>23&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M538.2032,-257.8722C534.8316,-252.2105 531.1955,-245.9014 528.0591,-240 524.2785,-232.8864 520.4337,-225.0669 516.9726,-217.7676\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"519.9328,-215.8341 512.5329,-208.2517 513.5892,-218.7937 519.9328,-215.8341\"/>\n",
       "<text text-anchor=\"middle\" x=\"550.5454\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">auxpass</text>\n",
       "</g>\n",
       "<!-- 22 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>22</title>\n",
       "<text text-anchor=\"middle\" x=\"594.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">22 (fatally)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;22 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>23&#45;&gt;22</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M561.88,-257.643C565.455,-252.0813 569.2329,-245.8848 572.3833,-240 576.1375,-232.9872 579.7933,-225.2024 583.0121,-217.9043\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"586.3752,-218.9423 587.0989,-208.3722 579.9415,-216.1839 586.3752,-218.9423\"/>\n",
       "<text text-anchor=\"middle\" x=\"601.9351\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>26</title>\n",
       "<text text-anchor=\"middle\" x=\"707.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">26 (Zimmerman)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;26 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>23&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M582.5441,-262.0972C596.8621,-255.7864 613.6686,-247.961 628.3833,-240 643.1389,-232.0169 658.8574,-222.3295 672.3559,-213.6153\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"674.4115,-216.4533 680.8734,-208.0572 670.586,-210.5911 674.4115,-216.4533\"/>\n",
       "<text text-anchor=\"middle\" x=\"667.3281\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 36 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>36</title>\n",
       "<text text-anchor=\"middle\" x=\"856.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">36 (Florida)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;36 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>23&#45;&gt;36</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M582.6464,-267.6451C610.6488,-260.5344 651.722,-249.924 687.3833,-240 726.7862,-229.0348 771.1175,-215.8883 804.4873,-205.8282\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"805.9136,-209.0536 814.474,-202.811 803.889,-202.3527 805.9136,-209.0536\"/>\n",
       "<text text-anchor=\"middle\" x=\"750.3281\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>16</title>\n",
       "<text text-anchor=\"middle\" x=\"391.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">16 (at)</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>17</title>\n",
       "<text text-anchor=\"middle\" x=\"472.3833\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">17 (17)</text>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>18</title>\n",
       "<text text-anchor=\"middle\" x=\"472.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">18 (years)</text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;17 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>18&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M472.3833,-85.7616C472.3833,-74.3597 472.3833,-59.4342 472.3833,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"475.8834,-46.2121 472.3833,-36.2121 468.8834,-46.2121 475.8834,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"497.2729\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nummod</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;16 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>19&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M416.5969,-171.7616C412.2695,-160.1316 406.578,-144.8357 401.6977,-131.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"404.9276,-130.3637 398.1599,-122.2121 398.367,-132.8049 404.9276,-130.3637\"/>\n",
       "<text text-anchor=\"middle\" x=\"421.4282\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;18 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>19&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M433.7749,-171.7616C440.5313,-159.9036 449.459,-144.2345 457.0275,-130.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"460.0971,-132.6334 462.0066,-122.2121 454.0151,-129.1681 460.0971,-132.6334\"/>\n",
       "<text text-anchor=\"middle\" x=\"488.7178\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod:npmod</text>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>24</title>\n",
       "<text text-anchor=\"middle\" x=\"567.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">24 (by)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;24 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>26&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M668.1869,-171.9496C657.328,-166.5419 645.6763,-160.3501 635.2935,-154 622.6297,-146.2549 609.3446,-136.8017 597.9168,-128.2084\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"600.0333,-125.4208 589.9593,-122.1398 595.7884,-130.9869 600.0333,-125.4208\"/>\n",
       "<text text-anchor=\"middle\" x=\"647.4282\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>25</title>\n",
       "<text text-anchor=\"middle\" x=\"655.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">25 (George)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;25 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>26&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M684.2392,-171.951C678.6851,-166.6739 673.2261,-160.5473 669.2798,-154 665.2509,-147.3157 662.3575,-139.4301 660.2923,-131.9215\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"663.6687,-130.9902 657.9608,-122.0643 656.8567,-132.6015 663.6687,-130.9902\"/>\n",
       "<text text-anchor=\"middle\" x=\"698.9351\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 31 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>31</title>\n",
       "<text text-anchor=\"middle\" x=\"763.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">31 (volunteer)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;31 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>26&#45;&gt;31</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M719.2595,-171.7616C726.981,-159.9036 737.1841,-144.2345 745.8338,-130.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"749.0005,-132.502 751.5243,-122.2121 743.1345,-128.6822 749.0005,-132.502\"/>\n",
       "<text text-anchor=\"middle\" x=\"755.7144\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">appos</text>\n",
       "</g>\n",
       "<!-- 33 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>33</title>\n",
       "<text text-anchor=\"middle\" x=\"856.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">33 (in)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;33 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>36&#45;&gt;33</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M856.3833,-171.7616C856.3833,-160.3597 856.3833,-145.4342 856.3833,-132.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"859.8834,-132.2121 856.3833,-122.2121 852.8834,-132.2121 859.8834,-132.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"868.4282\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 34 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>34</title>\n",
       "<text text-anchor=\"middle\" x=\"945.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">34 (Sanford)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;34 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>36&#45;&gt;34</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M875.2579,-171.7616C888.0016,-159.4475 904.9985,-143.0235 919.067,-129.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"921.7767,-131.6779 926.5359,-122.2121 916.9125,-126.644 921.7767,-131.6779\"/>\n",
       "<text text-anchor=\"middle\" x=\"934.9351\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>28</title>\n",
       "<text text-anchor=\"middle\" x=\"658.3833\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">28 (a)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;28 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>31&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M741.3719,-85.9716C725.9715,-73.358 705.2148,-56.3573 688.313,-42.5139\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"690.3198,-39.6334 680.3658,-36.0047 685.8843,-45.0488 690.3198,-39.6334\"/>\n",
       "<text text-anchor=\"middle\" x=\"725.9351\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>29</title>\n",
       "<text text-anchor=\"middle\" x=\"763.3833\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">29 (neighborhood)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;29 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>31&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M763.3833,-85.7616C763.3833,-74.3597 763.3833,-59.4342 763.3833,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"766.8834,-46.2121 763.3833,-36.2121 759.8834,-46.2121 766.8834,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"792.9351\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 30 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>30</title>\n",
       "<text text-anchor=\"middle\" x=\"879.3833\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">30 (watch)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;30 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>31&#45;&gt;30</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M798.3154,-85.9914C807.713,-80.629 817.6789,-74.4499 826.3833,-68 836.2881,-60.6606 846.3323,-51.6634 854.9744,-43.3341\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"857.5027,-45.7565 862.1694,-36.2448 852.5896,-40.7703 857.5027,-45.7565\"/>\n",
       "<text text-anchor=\"middle\" x=\"873.9351\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x110397048>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    graph = graphviz.Source(list(depParses[3])[0].to_dot())\n",
    "except IndexError:\n",
    "    print(\"You likely have to rerun the depParses\")\n",
    "    raise\n",
    "except:\n",
    "    graph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do a dependency parse on the reddit sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topPostDepParse = list(stanford.depParser.parse_sents(redditTopScores['sentences'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes a few seconds, but now lets look at the parse tree from one of the processed sentences.\n",
    "\n",
    "The sentence is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So anyway , I get a call from an older gentleman who 's quite bitter and mean right off the bat ( does n't like that I asked for his address / telephone number to verify the account , hates that he has to speak with a machine before reaching an agent , etc . ) .\n"
     ]
    }
   ],
   "source": [
    "targetSentence = 7\n",
    "print(' '.join(redditTopScores['sentences'][0][targetSentence]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which leads to a very rich dependancy tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"1113pt\" height=\"818pt\"\n",
       " viewBox=\"0.00 0.00 1112.57 818.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 814)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-814 1108.5708,-814 1108.5708,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"197\" y=\"-787.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"197\" y=\"-701.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">5 (get)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;5 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M197,-773.7616C197,-762.3597 197,-747.4342 197,-734.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"200.5001,-734.2121 197,-724.2121 193.5001,-734.2121 200.5001,-734.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"208.2759\" y=\"-744.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-615.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (So)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>5&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M169.8559,-700.0143C147.0611,-694.3052 114.1086,-684.4249 87.8965,-670 75.603,-663.2347 63.384,-653.8632 53.1537,-645.0603\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"55.2127,-642.2079 45.4121,-638.1831 50.5637,-647.4412 55.2127,-642.2079\"/>\n",
       "<text text-anchor=\"middle\" x=\"110.5518\" y=\"-658.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"112\" y=\"-615.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (anyway)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>5&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M169.7699,-688.3776C162.2887,-682.9299 154.4412,-676.6103 147.8965,-670 140.9396,-662.9735 134.3222,-654.4941 128.7076,-646.5301\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"131.5534,-644.4907 123.0402,-638.1838 125.7623,-648.423 131.5534,-644.4907\"/>\n",
       "<text text-anchor=\"middle\" x=\"170.5518\" y=\"-658.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"197\" y=\"-615.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (I)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>5&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M197,-687.7616C197,-676.3597 197,-661.4342 197,-648.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"200.5001,-648.2121 197,-638.2121 193.5001,-648.2121 200.5001,-648.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"212.1689\" y=\"-658.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"270\" y=\"-615.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">7 (call)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;7 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>5&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M214.6382,-687.9935C220.0064,-682.3367 225.8468,-675.9998 231,-670 237.4265,-662.5177 244.1316,-654.1485 250.1331,-646.4296\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"253.0006,-648.4423 256.3228,-638.3818 247.4519,-644.1747 253.0006,-648.4423\"/>\n",
       "<text text-anchor=\"middle\" x=\"255.4448\" y=\"-658.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dobj</text>\n",
       "</g>\n",
       "<!-- 34 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>34</title>\n",
       "<text text-anchor=\"middle\" x=\"643\" y=\"-615.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">34 (number)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;34 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;34</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M224.2279,-700.7498C296.5806,-686.7984 494.6077,-648.6138 590.1104,-630.1985\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"590.9741,-633.5965 600.1305,-628.2663 589.6487,-626.7231 590.9741,-633.5965\"/>\n",
       "<text text-anchor=\"middle\" x=\"457.1069\" y=\"-658.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dep</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"175\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">6 (a)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;6 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>7&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M249.8529,-601.7616C236.1242,-589.3335 217.771,-572.719 202.6742,-559.0524\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"204.8805,-556.3286 195.118,-552.2121 200.1826,-561.518 204.8805,-556.3286\"/>\n",
       "<text text-anchor=\"middle\" x=\"237.5518\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>11</title>\n",
       "<text text-anchor=\"middle\" x=\"270\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">11 (gentleman)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;11 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M270,-601.7616C270,-590.3597 270,-575.4342 270,-562.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"273.5001,-562.2121 270,-552.2121 266.5001,-562.2121 273.5001,-562.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"285.9448\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>25</title>\n",
       "<text text-anchor=\"middle\" x=\"569\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">25 (like)</text>\n",
       "</g>\n",
       "<!-- 34&#45;&gt;25 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>34&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M627.3065,-601.7616C616.8088,-589.5615 602.8398,-573.3273 591.2066,-559.8076\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"593.8464,-557.5094 584.6709,-552.2121 588.5403,-562.0751 593.8464,-557.5094\"/>\n",
       "<text text-anchor=\"middle\" x=\"621.1069\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dep</text>\n",
       "</g>\n",
       "<!-- 33 -->\n",
       "<g id=\"node32\" class=\"node\">\n",
       "<title>33</title>\n",
       "<text text-anchor=\"middle\" x=\"668\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">33 (telephone)</text>\n",
       "</g>\n",
       "<!-- 34&#45;&gt;33 -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>34&#45;&gt;33</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M648.3019,-601.7616C651.6495,-590.2456 656.0421,-575.1353 659.8295,-562.1064\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"663.2752,-562.7916 662.7058,-552.2121 656.5534,-560.8376 663.2752,-562.7916\"/>\n",
       "<text text-anchor=\"middle\" x=\"686.5518\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 36 -->\n",
       "<g id=\"node33\" class=\"node\">\n",
       "<title>36</title>\n",
       "<text text-anchor=\"middle\" x=\"780\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">36 (verify)</text>\n",
       "</g>\n",
       "<!-- 34&#45;&gt;36 -->\n",
       "<g id=\"edge32\" class=\"edge\">\n",
       "<title>34&#45;&gt;36</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M685.5984,-602.4626C697.0839,-597.1021 709.2884,-590.8061 720,-584 731.5591,-576.6554 743.3366,-567.3235 753.361,-558.7155\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"755.7821,-561.2474 760.9868,-552.019 751.1633,-555.9874 755.7821,-561.2474\"/>\n",
       "<text text-anchor=\"middle\" x=\"748.1587\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">acl</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"112\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">8 (from)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;8 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>11&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M231.7907,-515.847C220.4724,-510.291 208.113,-504.0465 196.9102,-498 181.4511,-489.6562 164.732,-479.9612 150.251,-471.3415\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"151.9329,-468.2691 141.5551,-466.1352 148.3371,-474.275 151.9329,-468.2691\"/>\n",
       "<text text-anchor=\"middle\" x=\"209.0449\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"189\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">9 (an)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;9 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>11&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M252.822,-515.7616C241.3312,-503.5615 226.0409,-487.3273 213.3072,-473.8076\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"215.5574,-471.0919 206.1533,-466.2121 210.4617,-475.8913 215.5574,-471.0919\"/>\n",
       "<text text-anchor=\"middle\" x=\"243.5518\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"270\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">10 (older)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;10 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>11&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M270,-515.7616C270,-504.3597 270,-489.4342 270,-476.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"273.5001,-476.2121 270,-466.2121 266.5001,-476.2121 273.5001,-476.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"285.5518\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>15</title>\n",
       "<text text-anchor=\"middle\" x=\"360\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">15 (bitter)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;15 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>11&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M289.0867,-515.7616C301.9735,-503.4475 319.1614,-487.0235 333.388,-473.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"336.1289,-475.6512 340.9408,-466.2121 331.2929,-470.5902 336.1289,-475.6512\"/>\n",
       "<text text-anchor=\"middle\" x=\"343.5381\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">acl:relcl</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"160\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">12 (who)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;12 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>15&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M323.7668,-433.1476C320.814,-432.047 317.8649,-430.9839 315,-430 287.7259,-420.6334 279.3566,-422.9093 252.6621,-412 234.2444,-404.4732 214.7375,-394.3082 198.3969,-385.109\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"200.0902,-382.0455 189.6716,-380.1206 196.6158,-388.1224 200.0902,-382.0455\"/>\n",
       "<text text-anchor=\"middle\" x=\"268.1689\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>13</title>\n",
       "<text text-anchor=\"middle\" x=\"238\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">13 (&#39;s)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;13 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>15&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M332.2421,-429.9468C323.6973,-424.2881 314.3008,-417.9619 305.7861,-412 294.0448,-403.7789 281.3575,-394.5234 270.1697,-386.2309\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"272.0587,-383.2738 261.9472,-380.1103 267.8789,-388.8889 272.0587,-383.2738\"/>\n",
       "<text text-anchor=\"middle\" x=\"316.1069\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cop</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>14</title>\n",
       "<text text-anchor=\"middle\" x=\"318\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">14 (quite)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;14 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>15&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M346.5824,-429.8451C342.8269,-424.2891 338.9433,-418.045 335.8965,-412 332.3589,-404.9812 329.1928,-397.0923 326.543,-389.6912\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"329.7918,-388.373 323.2634,-380.0286 323.1632,-390.6229 329.7918,-388.373\"/>\n",
       "<text text-anchor=\"middle\" x=\"358.5518\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>16</title>\n",
       "<text text-anchor=\"middle\" x=\"403\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">16 (and)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;16 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>15&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M371.0917,-429.9088C374.4154,-424.2486 377.9776,-417.9311 381,-412 384.6116,-404.9127 388.2182,-397.1023 391.4354,-389.8033\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"394.7951,-390.8514 395.5451,-380.2831 388.3684,-388.0771 394.7951,-390.8514\"/>\n",
       "<text text-anchor=\"middle\" x=\"394.2139\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cc</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>17</title>\n",
       "<text text-anchor=\"middle\" x=\"489\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">17 (mean)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;17 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>15&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M387.0426,-429.9716C406.393,-417.0713 432.6266,-399.5822 453.6376,-385.575\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"455.6139,-388.4639 461.993,-380.0047 451.731,-382.6395 455.6139,-388.4639\"/>\n",
       "<text text-anchor=\"middle\" x=\"445.0518\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">conj</text>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>18</title>\n",
       "<text text-anchor=\"middle\" x=\"451\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">18 (right)</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;18 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>17&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M470.6407,-343.8974C466.1437,-338.5346 461.7952,-332.3788 458.8965,-326 455.8322,-319.2569 453.9297,-311.5343 452.7561,-304.2081\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"456.2273,-303.7587 451.5373,-294.2585 449.2792,-304.6099 456.2273,-303.7587\"/>\n",
       "<text text-anchor=\"middle\" x=\"481.5518\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>21</title>\n",
       "<text text-anchor=\"middle\" x=\"534\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">21 (bat)</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;21 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>17&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M498.5433,-343.7616C504.6885,-332.0176 512.7896,-316.5355 519.6969,-303.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"522.9353,-304.6951 524.4704,-294.2121 516.733,-301.4497 522.9353,-304.6951\"/>\n",
       "<text text-anchor=\"middle\" x=\"530.9448\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>19</title>\n",
       "<text text-anchor=\"middle\" x=\"478\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">19 (off)</text>\n",
       "</g>\n",
       "<!-- 21&#45;&gt;19 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>21&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M522.1238,-257.7616C514.4023,-245.9036 504.1992,-230.2345 495.5495,-216.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"498.2488,-214.6822 489.859,-208.2121 492.3828,-218.502 498.2488,-214.6822\"/>\n",
       "<text text-anchor=\"middle\" x=\"522.0449\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 20 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>20</title>\n",
       "<text text-anchor=\"middle\" x=\"555\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">20 (the)</text>\n",
       "</g>\n",
       "<!-- 21&#45;&gt;20 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>21&#45;&gt;20</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M538.4536,-257.7616C541.2656,-246.2456 544.9553,-231.1353 548.1368,-218.1064\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"551.5807,-218.757 550.5529,-208.2121 544.7805,-217.0964 551.5807,-218.757\"/>\n",
       "<text text-anchor=\"middle\" x=\"554.5518\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>23</title>\n",
       "<text text-anchor=\"middle\" x=\"489\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">23 (does)</text>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>24</title>\n",
       "<text text-anchor=\"middle\" x=\"569\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">24 (n&#39;t)</text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;23 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>25&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M552.0341,-515.7616C540.6851,-503.5615 525.5836,-487.3273 513.0071,-473.8076\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"515.3152,-471.1501 505.9415,-466.2121 510.1899,-475.9179 515.3152,-471.1501\"/>\n",
       "<text text-anchor=\"middle\" x=\"544.1069\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">aux</text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;24 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>25&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M569,-515.7616C569,-504.3597 569,-489.4342 569,-476.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"572.5001,-476.2121 569,-466.2121 565.5001,-476.2121 572.5001,-476.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"579.1069\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">neg</text>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>28</title>\n",
       "<text text-anchor=\"middle\" x=\"652\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">28 (asked)</text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;28 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>25&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M586.6022,-515.7616C598.4867,-503.4475 614.3377,-487.0235 627.4578,-473.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"629.9971,-475.8382 634.4232,-466.2121 624.9603,-470.977 629.9971,-475.8382\"/>\n",
       "<text text-anchor=\"middle\" x=\"634.6587\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ccomp</text>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>26</title>\n",
       "<text text-anchor=\"middle\" x=\"575\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">26 (that)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;26 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>28&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M635.2812,-429.7312C630.1192,-424.0637 624.4271,-417.7868 619.2344,-412 612.1517,-404.1071 604.4811,-395.4688 597.5476,-387.6264\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"600.1509,-385.2867 590.9089,-380.1063 594.9032,-389.9194 600.1509,-385.2867\"/>\n",
       "<text text-anchor=\"middle\" x=\"634.3828\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 27 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>27</title>\n",
       "<text text-anchor=\"middle\" x=\"652\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">27 (I)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;27 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>28&#45;&gt;27</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M652,-429.7616C652,-418.3597 652,-403.4342 652,-390.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"655.5001,-390.2121 652,-380.2121 648.5001,-390.2121 655.5001,-390.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"667.1689\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 31 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>31</title>\n",
       "<text text-anchor=\"middle\" x=\"739\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">31 (address)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;31 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>28&#45;&gt;31</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M670.4505,-429.7616C682.9077,-417.4475 699.5227,-401.0235 713.275,-387.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"715.9248,-389.7314 720.5761,-380.2121 711.0037,-384.7531 715.9248,-389.7314\"/>\n",
       "<text text-anchor=\"middle\" x=\"716.9448\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>29</title>\n",
       "<text text-anchor=\"middle\" x=\"662\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">29 (for)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;29 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>31&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M722.6703,-343.7616C711.747,-331.5615 697.2117,-315.3273 685.1068,-301.8076\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"687.5843,-299.3276 678.3062,-294.2121 682.3691,-303.9969 687.5843,-299.3276\"/>\n",
       "<text text-anchor=\"middle\" x=\"718.0449\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 30 -->\n",
       "<g id=\"node31\" class=\"node\">\n",
       "<title>30</title>\n",
       "<text text-anchor=\"middle\" x=\"739\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">30 (his)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;30 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>31&#45;&gt;30</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M739,-343.7616C739,-332.3597 739,-317.4342 739,-304.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"742.5001,-304.2121 739,-294.2121 735.5001,-304.2121 742.5001,-304.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"769.3379\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod:poss</text>\n",
       "</g>\n",
       "<!-- 35 -->\n",
       "<g id=\"node34\" class=\"node\">\n",
       "<title>35</title>\n",
       "<text text-anchor=\"middle\" x=\"780\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">35 (to)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;35 -->\n",
       "<g id=\"edge33\" class=\"edge\">\n",
       "<title>36&#45;&gt;35</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M780,-515.7616C780,-504.3597 780,-489.4342 780,-476.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"783.5001,-476.2121 780,-466.2121 776.5001,-476.2121 783.5001,-476.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"794.3828\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 38 -->\n",
       "<g id=\"node35\" class=\"node\">\n",
       "<title>38</title>\n",
       "<text text-anchor=\"middle\" x=\"883\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">38 (account)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;38 -->\n",
       "<g id=\"edge34\" class=\"edge\">\n",
       "<title>36&#45;&gt;38</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M801.8437,-515.7616C816.8651,-503.2195 836.9927,-486.4138 853.445,-472.677\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"855.7549,-475.3079 861.1878,-466.2121 851.2685,-469.9347 855.7549,-475.3079\"/>\n",
       "<text text-anchor=\"middle\" x=\"850.4448\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dobj</text>\n",
       "</g>\n",
       "<!-- 37 -->\n",
       "<g id=\"node36\" class=\"node\">\n",
       "<title>37</title>\n",
       "<text text-anchor=\"middle\" x=\"829\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">37 (the)</text>\n",
       "</g>\n",
       "<!-- 38&#45;&gt;37 -->\n",
       "<g id=\"edge35\" class=\"edge\">\n",
       "<title>38&#45;&gt;37</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M871.548,-429.7616C864.1023,-417.9036 854.2635,-402.2345 845.9227,-388.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"848.7174,-386.8198 840.4355,-380.2121 842.7891,-390.5422 848.7174,-386.8198\"/>\n",
       "<text text-anchor=\"middle\" x=\"868.5518\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 40 -->\n",
       "<g id=\"node37\" class=\"node\">\n",
       "<title>40</title>\n",
       "<text text-anchor=\"middle\" x=\"913\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">40 (hates)</text>\n",
       "</g>\n",
       "<!-- 38&#45;&gt;40 -->\n",
       "<g id=\"edge36\" class=\"edge\">\n",
       "<title>38&#45;&gt;40</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M889.3622,-429.7616C893.4192,-418.1316 898.755,-402.8357 903.3302,-389.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"906.6579,-390.8069 906.6469,-380.2121 900.0485,-388.5013 906.6579,-390.8069\"/>\n",
       "<text text-anchor=\"middle\" x=\"912.0518\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">conj</text>\n",
       "</g>\n",
       "<!-- 54 -->\n",
       "<g id=\"node38\" class=\"node\">\n",
       "<title>54</title>\n",
       "<text text-anchor=\"middle\" x=\"996\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">54 (etc)</text>\n",
       "</g>\n",
       "<!-- 38&#45;&gt;54 -->\n",
       "<g id=\"edge37\" class=\"edge\">\n",
       "<title>38&#45;&gt;54</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M906.6885,-429.9716C923.4129,-417.2433 946.007,-400.0478 964.285,-386.1371\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"966.5048,-388.8461 972.3427,-380.0047 962.2654,-383.2758 966.5048,-388.8461\"/>\n",
       "<text text-anchor=\"middle\" x=\"959.0518\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">conj</text>\n",
       "</g>\n",
       "<!-- 43 -->\n",
       "<g id=\"node39\" class=\"node\">\n",
       "<title>43</title>\n",
       "<text text-anchor=\"middle\" x=\"817\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">43 (has)</text>\n",
       "</g>\n",
       "<!-- 40&#45;&gt;43 -->\n",
       "<g id=\"edge38\" class=\"edge\">\n",
       "<title>40&#45;&gt;43</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M892.6409,-343.7616C878.7676,-331.3335 860.2213,-314.719 844.9655,-301.0524\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"847.1135,-298.2777 837.3298,-294.2121 842.4428,-303.4915 847.1135,-298.2777\"/>\n",
       "<text text-anchor=\"middle\" x=\"889.6587\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ccomp</text>\n",
       "</g>\n",
       "<!-- 50 -->\n",
       "<g id=\"node40\" class=\"node\">\n",
       "<title>50</title>\n",
       "<text text-anchor=\"middle\" x=\"974\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">50 (reaching)</text>\n",
       "</g>\n",
       "<!-- 40&#45;&gt;50 -->\n",
       "<g id=\"edge39\" class=\"edge\">\n",
       "<title>40&#45;&gt;50</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M925.9365,-343.7616C934.4283,-331.7896 945.6756,-315.9328 955.1549,-302.5685\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"958.1514,-304.3935 961.0821,-294.2121 952.4419,-300.3437 958.1514,-304.3935\"/>\n",
       "<text text-anchor=\"middle\" x=\"963.1587\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advcl</text>\n",
       "</g>\n",
       "<!-- 41 -->\n",
       "<g id=\"node41\" class=\"node\">\n",
       "<title>41</title>\n",
       "<text text-anchor=\"middle\" x=\"718\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">41 (that)</text>\n",
       "</g>\n",
       "<!-- 43&#45;&gt;41 -->\n",
       "<g id=\"edge40\" class=\"edge\">\n",
       "<title>43&#45;&gt;41</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M794.5653,-257.8876C787.6662,-252.2266 780.0866,-245.9139 773.2344,-240 763.9189,-231.9602 753.8992,-222.991 744.9852,-214.8934\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"747.228,-212.2018 737.4818,-208.047 742.5098,-217.3728 747.228,-212.2018\"/>\n",
       "<text text-anchor=\"middle\" x=\"788.3828\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 42 -->\n",
       "<g id=\"node42\" class=\"node\">\n",
       "<title>42</title>\n",
       "<text text-anchor=\"middle\" x=\"796\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">42 (he)</text>\n",
       "</g>\n",
       "<!-- 43&#45;&gt;42 -->\n",
       "<g id=\"edge41\" class=\"edge\">\n",
       "<title>43&#45;&gt;42</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M812.5464,-257.7616C809.7344,-246.2456 806.0447,-231.1353 802.8632,-218.1064\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"806.2195,-217.0964 800.4471,-208.2121 799.4193,-218.757 806.2195,-217.0964\"/>\n",
       "<text text-anchor=\"middle\" x=\"823.1689\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 45 -->\n",
       "<g id=\"node43\" class=\"node\">\n",
       "<title>45</title>\n",
       "<text text-anchor=\"middle\" x=\"879\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">45 (speak)</text>\n",
       "</g>\n",
       "<!-- 43&#45;&gt;45 -->\n",
       "<g id=\"edge42\" class=\"edge\">\n",
       "<title>43&#45;&gt;45</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M830.1486,-257.7616C838.7796,-245.7896 850.2112,-229.9328 859.8459,-216.5685\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"862.8614,-218.3707 865.8703,-208.2121 857.1832,-214.2771 862.8614,-218.3707\"/>\n",
       "<text text-anchor=\"middle\" x=\"871.0518\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">xcomp</text>\n",
       "</g>\n",
       "<!-- 49 -->\n",
       "<g id=\"node48\" class=\"node\">\n",
       "<title>49</title>\n",
       "<text text-anchor=\"middle\" x=\"974\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">49 (before)</text>\n",
       "</g>\n",
       "<!-- 50&#45;&gt;49 -->\n",
       "<g id=\"edge47\" class=\"edge\">\n",
       "<title>50&#45;&gt;49</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M974,-257.7616C974,-246.3597 974,-231.4342 974,-218.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"977.5001,-218.2121 974,-208.2121 970.5001,-218.2121 977.5001,-218.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"988.3828\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 52 -->\n",
       "<g id=\"node49\" class=\"node\">\n",
       "<title>52</title>\n",
       "<text text-anchor=\"middle\" x=\"1068\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">52 (agent)</text>\n",
       "</g>\n",
       "<!-- 50&#45;&gt;52 -->\n",
       "<g id=\"edge48\" class=\"edge\">\n",
       "<title>50&#45;&gt;52</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M993.935,-257.7616C1007.5192,-245.3335 1025.6792,-228.719 1040.6172,-215.0524\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1043.0782,-217.5446 1048.0938,-208.2121 1038.3531,-212.3799 1043.0782,-217.5446\"/>\n",
       "<text text-anchor=\"middle\" x=\"1039.4448\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dobj</text>\n",
       "</g>\n",
       "<!-- 44 -->\n",
       "<g id=\"node44\" class=\"node\">\n",
       "<title>44</title>\n",
       "<text text-anchor=\"middle\" x=\"844\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">44 (to)</text>\n",
       "</g>\n",
       "<!-- 45&#45;&gt;44 -->\n",
       "<g id=\"edge43\" class=\"edge\">\n",
       "<title>45&#45;&gt;44</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M871.4717,-171.6927C869.1458,-166.0237 866.5793,-159.7555 864.2344,-154 861.2709,-146.7261 858.0772,-138.8513 855.1223,-131.5502\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"858.2784,-130.0188 851.2849,-122.06 851.7888,-132.6429 858.2784,-130.0188\"/>\n",
       "<text text-anchor=\"middle\" x=\"879.3828\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 48 -->\n",
       "<g id=\"node45\" class=\"node\">\n",
       "<title>48</title>\n",
       "<text text-anchor=\"middle\" x=\"934\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">48 (machine)</text>\n",
       "</g>\n",
       "<!-- 45&#45;&gt;48 -->\n",
       "<g id=\"edge44\" class=\"edge\">\n",
       "<title>45&#45;&gt;48</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M890.6641,-171.7616C898.2477,-159.9036 908.2686,-144.2345 916.7639,-130.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"919.9135,-132.5223 922.3527,-122.2121 914.0163,-128.7509 919.9135,-132.5223\"/>\n",
       "<text text-anchor=\"middle\" x=\"925.9448\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 46 -->\n",
       "<g id=\"node46\" class=\"node\">\n",
       "<title>46</title>\n",
       "<text text-anchor=\"middle\" x=\"894\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">46 (with)</text>\n",
       "</g>\n",
       "<!-- 48&#45;&gt;46 -->\n",
       "<g id=\"edge45\" class=\"edge\">\n",
       "<title>48&#45;&gt;46</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M925.517,-85.7616C920.0547,-74.0176 912.8537,-58.5355 906.7139,-45.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"909.8616,-43.8032 902.4707,-36.2121 903.5146,-46.7554 909.8616,-43.8032\"/>\n",
       "<text text-anchor=\"middle\" x=\"929.0449\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 47 -->\n",
       "<g id=\"node47\" class=\"node\">\n",
       "<title>47</title>\n",
       "<text text-anchor=\"middle\" x=\"973\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">47 (a)</text>\n",
       "</g>\n",
       "<!-- 48&#45;&gt;47 -->\n",
       "<g id=\"edge46\" class=\"edge\">\n",
       "<title>48&#45;&gt;47</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M942.2709,-85.7616C947.5967,-74.0176 954.6176,-58.5355 960.604,-45.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"963.7985,-46.7649 964.741,-36.2121 957.4234,-43.8739 963.7985,-46.7649\"/>\n",
       "<text text-anchor=\"middle\" x=\"964.5518\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 51 -->\n",
       "<g id=\"node50\" class=\"node\">\n",
       "<title>51</title>\n",
       "<text text-anchor=\"middle\" x=\"1068\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">51 (an)</text>\n",
       "</g>\n",
       "<!-- 52&#45;&gt;51 -->\n",
       "<g id=\"edge49\" class=\"edge\">\n",
       "<title>52&#45;&gt;51</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1068,-171.7616C1068,-160.3597 1068,-145.4342 1068,-132.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1071.5001,-132.2121 1068,-122.2121 1064.5001,-132.2121 1071.5001,-132.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"1076.5518\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x1103cef28>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    graph = graphviz.Source(list(topPostDepParse[targetSentence])[0].to_dot())\n",
    "except IndexError:\n",
    "    print(\"You likely have to rerun the depParses\")\n",
    "    raise\n",
    "except:\n",
    "    graph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 3*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, parse a (modest) subset of your corpus of interest. How deep are the phrase structure and dependency parse trees nested? How does parse depth relate to perceived sentence complexity? What are five things you can extract from these parses for subsequent analysis? (e.g., nouns collocated in a noun phrase; adjectives that modify a noun; etc.) Capture these sets of things for a focal set of words (e.g., \"Bush\", \"Obama\", \"Trump\"). What do they reveal about the roles that these entities are perceived to play in the social world inscribed by your texts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How deep are the phrase structure and dependency parse trees nested?**\n",
    "\n",
    "<i>Quite deep, it goes a few level down. However, as the length of the sentence is not too long there are not too much that can be extracted here.</i>\n",
    "\n",
    "**How does parse depth relate to perceived sentence complexity?**\n",
    "\n",
    "<i>The sentence is quite complex as it involves numerous social actors ('Tjahjo Kumolo', 'Basuki Tjahaja', 'FPI') and it reveals many of their social roles ('governor', 'minister', 'organization').</i>\n",
    "\n",
    "**What are five things you can extract from these parses for subsequent analysis? (e.g., nouns collocated in a noun phrase; adjectives that modify a noun; etc.) Capture these sets of things for a focal set of words (e.g., \"Bush\", \"Obama\", \"Trump\"). What do they reveal about the roles that these entities are perceived to play in the social world inscribed by your texts?**\n",
    "<i>\n",
    "1. The main idea of the sentence can be found by looking at NP (noun phrase) and 'Tjahaja' + 'the' = Jakarta governor Basuki 'Ahok ' Tjahaja Purnama has issued a letter recommending the disbandment of FPI.\n",
    "\n",
    "2. Many tags reveal more or less similar things to the others. For example, NP + \"Thahaja\" and \"NP\" + \"FPI\" returns the same result despite having 2 different social actors. However, it might also be due to their dependency or connection to each other.\n",
    "\n",
    "3. With graph representation, the dependency seems to be more apparent. Many social roles are revealed as they are represented more intuitively. For example, it is revealed through word compounding that 'Tjahjo Kumolo' is a 'minister' or that the 'organization' (refers to FPI) is a 'hard-line' group (hard-line modifies organization).\n",
    "\n",
    "4. However, the graph identified 'Islamic Defender Front' and 'FPI' as 4 different things in different dependency levels instead of treating it as one entity.\n",
    "\n",
    "5. It's interesting to look at branch number 27 in the graph where the word 'recommending' branches out into 'letter' and 'disbandment'. The word 'letter' leads to Ahok, which signals that he is the one who wrote the letter, and the word 'disbandment' branches out to the 'hard-line organization' (referring to FPI) as the subject in Ahok's letter.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home Minister Tjahjo Kumolo has said that his ministry is ready to follow up on acting Jakarta governor Basuki 'Ahok' Tjahaja Purnama's letter recommending the disbandment of hard-line organization the Islam Defenders Front (FPI).\n",
      "He explained that his ministry would follow up on the letter because the government should listen to every voice of the people and institutions in the archipelago.\n",
      "n Monday, Ahok, a Christian of Chinese descent, said that he would file the recommendation letter because the FPI's protests were offensive and in contravention of the 1945 Constitution.\n"
     ]
    }
   ],
   "source": [
    "article = [\"Home Minister Tjahjo Kumolo has said that his ministry is ready to follow up on acting Jakarta governor Basuki 'Ahok' Tjahaja Purnama's letter recommending the disbandment of hard-line organization the Islam Defenders Front (FPI).\", \"He explained that his ministry would follow up on the letter because the government should listen to every voice of the people and institutions in the archipelago.\", \"n Monday, Ahok, a Christian of Chinese descent, said that he would file the recommendation letter because the FPI's protests were offensive and in contravention of the 1945 Constitution.\"]\n",
    "\n",
    "tokenized_article = [nltk.word_tokenize(t) for t in article]\n",
    "print('\\n'.join(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('NNP', ['Home']), Tree('NNP', ['Minister']), Tree('NNP', ['Tjahjo']), Tree('NNP', ['Kumolo'])]), Tree('VP', [Tree('VBZ', ['has']), Tree('VP', [Tree('VBN', ['said']), Tree('SBAR', [Tree('IN', ['that']), Tree('S', [Tree('NP', [Tree('PRP$', ['his']), Tree('NN', ['ministry'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('ADJP', [Tree('JJ', ['ready']), Tree('S', [Tree('VP', [Tree('TO', ['to']), Tree('VP', [Tree('VB', ['follow']), Tree('PRT', [Tree('RP', ['up'])]), Tree('PP', [Tree('IN', ['on']), Tree('S', [Tree('VP', [Tree('VBG', ['acting']), Tree('NP', [Tree('NP', [Tree('NP', [Tree('NP', [Tree('NNP', ['Jakarta']), Tree('NNP', ['governor']), Tree('NNP', ['Basuki']), Tree('NN', [\"'Ahok\"]), Tree('POS', [\"'\"])]), Tree('NNP', ['Tjahaja']), Tree('NNP', ['Purnama']), Tree('POS', [\"'s\"])]), Tree('NN', ['letter'])]), Tree('VP', [Tree('VBG', ['recommending']), Tree('NP', [Tree('NP', [Tree('DT', ['the']), Tree('NN', ['disbandment'])]), Tree('PP', [Tree('IN', ['of']), Tree('NP', [Tree('NP', [Tree('JJ', ['hard-line']), Tree('NN', ['organization'])]), Tree('NP', [Tree('DT', ['the']), Tree('NNP', ['Islam']), Tree('NNP', ['Defenders']), Tree('NNP', ['Front'])])])])])]), Tree('PRN', [Tree('-LRB-', [Tree('', []), Tree('NP', [Tree('NNP', ['FPI'])]), Tree('-RRB-', [])])])])])])])])])])])])])])])]), Tree('.', ['.'])])])]\n"
     ]
    }
   ],
   "source": [
    "parse = list(stanford.parser.parse_sents(tokenized_article)) #Converting the iterator to a list so we can call by index. They are still \n",
    "firstSentParseTree = list(parse[0]) #iterators so be careful about re-running code, without re-running this block\n",
    "print(firstSentParseTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('NP',\n",
       "   \"Jakarta governor Basuki 'Ahok ' Tjahaja Purnama 's letter recommending the disbandment of hard-line organization the Islam Defenders Front FPI\")]]"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeRelation(firstSentParseTree, 'NP', 'Tjahaja', 'the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('VP',\n",
       "   \"has said that his ministry is ready to follow up on acting Jakarta governor Basuki 'Ahok ' Tjahaja Purnama 's letter recommending the disbandment of hard-line organization the Islam Defenders Front FPI\")],\n",
       " [('VP',\n",
       "   \"said that his ministry is ready to follow up on acting Jakarta governor Basuki 'Ahok ' Tjahaja Purnama 's letter recommending the disbandment of hard-line organization the Islam Defenders Front FPI\")],\n",
       " [('VP',\n",
       "   \"is ready to follow up on acting Jakarta governor Basuki 'Ahok ' Tjahaja Purnama 's letter recommending the disbandment of hard-line organization the Islam Defenders Front FPI\")],\n",
       " [('VP',\n",
       "   \"to follow up on acting Jakarta governor Basuki 'Ahok ' Tjahaja Purnama 's letter recommending the disbandment of hard-line organization the Islam Defenders Front FPI\")],\n",
       " [('VP',\n",
       "   \"follow up on acting Jakarta governor Basuki 'Ahok ' Tjahaja Purnama 's letter recommending the disbandment of hard-line organization the Islam Defenders Front FPI\")],\n",
       " [('VP',\n",
       "   \"acting Jakarta governor Basuki 'Ahok ' Tjahaja Purnama 's letter recommending the disbandment of hard-line organization the Islam Defenders Front FPI\")]]"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeRelation(firstSentParseTree, 'VP', 'Basuki', 'the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('NP',\n",
       "   \"Jakarta governor Basuki 'Ahok ' Tjahaja Purnama 's letter recommending the disbandment of hard-line organization the Islam Defenders Front FPI\")]]"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeRelation(firstSentParseTree, 'NP', 'FPI', 'the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('ADJP',\n",
       "   \"ready to follow up on acting Jakarta governor Basuki 'Ahok ' Tjahaja Purnama 's letter recommending the disbandment of hard-line organization the Islam Defenders Front FPI\")]]"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeRelation(firstSentParseTree, 'ADJP', 'FPI', 'the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                          ROOT                                                                                                                                             \n",
      "                                                                                                                           |                                                                                                                                                \n",
      "                                                                                                                           S                                                                                                                                               \n",
      "         __________________________________________________________________________________________________________________|_____________________________________________________________________________________________________________________________________________   \n",
      "        |                        VP                                                                                                                                                                                                                                      | \n",
      "        |                     ___|____                                                                                                                                                                                                                                   |  \n",
      "        |                    |        VP                                                                                                                                                                                                                                 | \n",
      "        |                    |    ____|____                                                                                                                                                                                                                              |  \n",
      "        |                    |   |        SBAR                                                                                                                                                                                                                           | \n",
      "        |                    |   |     ____|__________                                                                                                                                                                                                                   |  \n",
      "        |                    |   |    |               S                                                                                                                                                                                                                  | \n",
      "        |                    |   |    |          _____|___________                                                                                                                                                                                                       |  \n",
      "        |                    |   |    |         |                 VP                                                                                                                                                                                                     | \n",
      "        |                    |   |    |         |             ____|____                                                                                                                                                                                                  |  \n",
      "        |                    |   |    |         |            |        ADJP                                                                                                                                                                                               | \n",
      "        |                    |   |    |         |            |     ____|_____                                                                                                                                                                                            |  \n",
      "        |                    |   |    |         |            |    |          S                                                                                                                                                                                           | \n",
      "        |                    |   |    |         |            |    |          |                                                                                                                                                                                           |  \n",
      "        |                    |   |    |         |            |    |          VP                                                                                                                                                                                          | \n",
      "        |                    |   |    |         |            |    |     _____|_________                                                                                                                                                                                  |  \n",
      "        |                    |   |    |         |            |    |    |               VP                                                                                                                                                                                | \n",
      "        |                    |   |    |         |            |    |    |      _________|____________________                                                                                                                                                             |  \n",
      "        |                    |   |    |         |            |    |    |     |     |                        PP                                                                                                                                                           | \n",
      "        |                    |   |    |         |            |    |    |     |     |    ____________________|___________________                                                                                                                                         |  \n",
      "        |                    |   |    |         |            |    |    |     |     |   |                                        S                                                                                                                                        | \n",
      "        |                    |   |    |         |            |    |    |     |     |   |                                        |                                                                                                                                        |  \n",
      "        |                    |   |    |         |            |    |    |     |     |   |                                        VP                                                                                                                                       | \n",
      "        |                    |   |    |         |            |    |    |     |     |   |     ___________________________________|___________________________________________                                                                                             |  \n",
      "        |                    |   |    |         |            |    |    |     |     |   |    |                                                                               NP                                                                                           | \n",
      "        |                    |   |    |         |            |    |    |     |     |   |    |                                    ___________________________________________|_________________________________________________________________________________           |  \n",
      "        |                    |   |    |         |            |    |    |     |     |   |    |                                   |                                           VP                                                                                |          | \n",
      "        |                    |   |    |         |            |    |    |     |     |   |    |                                   |                                   ________|___________                                                                      |          |  \n",
      "        |                    |   |    |         |            |    |    |     |     |   |    |                                   |                                  |                    NP                                                                    |          | \n",
      "        |                    |   |    |         |            |    |    |     |     |   |    |                                   |                                  |             _______|______________                                                       |          |  \n",
      "        |                    |   |    |         |            |    |    |     |     |   |    |                                   NP                                 |            |                      PP                                                    PRN         | \n",
      "        |                    |   |    |         |            |    |    |     |     |   |    |                               ____|________________________          |            |                ______|_______________________                               |          |  \n",
      "        |                    |   |    |         |            |    |    |     |     |   |    |                              NP                            |         |            |               |                              NP                           -LRB-        | \n",
      "        |                    |   |    |         |            |    |    |     |     |   |    |                        ______|________________________     |         |            |               |              ________________|____                      ____|_____     |  \n",
      "        NP                   |   |    |         NP           |    |    |     |    PRT  |    |                       NP                |       |     |    |         |            NP              |             NP                    NP                   |    NP    |    | \n",
      "  ______|______________      |   |    |     ____|_____       |    |    |     |     |   |    |        _______________|___________      |       |     |    |         |         ___|_______        |       ______|_______          ____|_______________     |    |     |    |  \n",
      "NNP    NNP     NNP    NNP   VBZ VBN   IN  PRP$        NN    VBZ   JJ   TO    VB    RP  IN  VBG     NNP     NNP     NNP     NN  POS   NNP     NNP   POS   NN       VBG       DT          NN      IN     JJ             NN       DT  NNP     NNP     NNP       NNP  -RRB-  . \n",
      " |      |       |      |     |   |    |    |          |      |    |    |     |     |   |    |       |       |       |      |    |     |       |     |    |         |        |           |       |      |              |        |    |       |       |    |    |     |    |  \n",
      "Home Minister Tjahjo Kumolo has said that his      ministry  is ready  to  follow  up  on acting Jakarta governor Basuki 'Ahok  '  Tjahaja Purnama  's letter recommending the     disbandment  of hard-line     organization the Islam Defenders Front ...  FPI   ...   . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "firstSentParseTree[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dependency parsing and graph representations</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function DependencyGraph.__init__.<locals>.<lambda> at 0x110c3c6a8>,\n",
      "            {0: {'address': 0,\n",
      "                 'ctag': 'TOP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'root': [6]}),\n",
      "                 'feats': None,\n",
      "                 'head': None,\n",
      "                 'lemma': None,\n",
      "                 'rel': None,\n",
      "                 'tag': 'TOP',\n",
      "                 'word': None},\n",
      "             1: {'address': 1,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'compound',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'Home'},\n",
      "             2: {'address': 2,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'compound',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'Minister'},\n",
      "             3: {'address': 3,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'compound',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'Tjahjo'},\n",
      "             4: {'address': 4,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'compound': [1, 2, 3]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 5,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'nsubj',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'Kumolo'},\n",
      "             5: {'address': 5,\n",
      "                 'ctag': 'VBZ',\n",
      "                 'deps': defaultdict(<class 'list'>, {'nsubj': [4]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 6,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'dep',\n",
      "                 'tag': 'VBZ',\n",
      "                 'word': 'has'},\n",
      "             6: {'address': 6,\n",
      "                 'ctag': 'VBD',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'ccomp': [11],\n",
      "                                      'dep': [5],\n",
      "                                      'nsubj': [36]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 0,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'root',\n",
      "                 'tag': 'VBD',\n",
      "                 'word': 'said'},\n",
      "             7: {'address': 7,\n",
      "                 'ctag': 'IN',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 11,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'mark',\n",
      "                 'tag': 'IN',\n",
      "                 'word': 'that'},\n",
      "             8: {'address': 8,\n",
      "                 'ctag': 'PRP$',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 9,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'nmod:poss',\n",
      "                 'tag': 'PRP$',\n",
      "                 'word': 'his'},\n",
      "             9: {'address': 9,\n",
      "                 'ctag': 'NN',\n",
      "                 'deps': defaultdict(<class 'list'>, {'nmod:poss': [8]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 11,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'nsubj',\n",
      "                 'tag': 'NN',\n",
      "                 'word': 'ministry'},\n",
      "             10: {'address': 10,\n",
      "                  'ctag': 'VBZ',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 11,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'cop',\n",
      "                  'tag': 'VBZ',\n",
      "                  'word': 'is'},\n",
      "             11: {'address': 11,\n",
      "                  'ctag': 'JJ',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'cop': [10],\n",
      "                                       'mark': [7],\n",
      "                                       'nsubj': [9],\n",
      "                                       'xcomp': [13]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 6,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'ccomp',\n",
      "                  'tag': 'JJ',\n",
      "                  'word': 'ready'},\n",
      "             12: {'address': 12,\n",
      "                  'ctag': 'TO',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 13,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'mark',\n",
      "                  'tag': 'TO',\n",
      "                  'word': 'to'},\n",
      "             13: {'address': 13,\n",
      "                  'ctag': 'VB',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'advcl': [16],\n",
      "                                       'compound:prt': [14],\n",
      "                                       'mark': [12]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 11,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'xcomp',\n",
      "                  'tag': 'VB',\n",
      "                  'word': 'follow'},\n",
      "             14: {'address': 14,\n",
      "                  'ctag': 'RP',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 13,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'compound:prt',\n",
      "                  'tag': 'RP',\n",
      "                  'word': 'up'},\n",
      "             15: {'address': 15,\n",
      "                  'ctag': 'IN',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 16,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'mark',\n",
      "                  'tag': 'IN',\n",
      "                  'word': 'on'},\n",
      "             16: {'address': 16,\n",
      "                  'ctag': 'VBG',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'advcl': [27],\n",
      "                                       'dobj': [19],\n",
      "                                       'mark': [15]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 13,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'advcl',\n",
      "                  'tag': 'VBG',\n",
      "                  'word': 'acting'},\n",
      "             17: {'address': 17,\n",
      "                  'ctag': 'NNP',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 19,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'compound',\n",
      "                  'tag': 'NNP',\n",
      "                  'word': 'Jakarta'},\n",
      "             18: {'address': 18,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 19,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'compound',\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'governor'},\n",
      "             19: {'address': 19,\n",
      "                  'ctag': 'NNP',\n",
      "                  'deps': defaultdict(<class 'list'>, {'compound': [17, 18]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 16,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'dobj',\n",
      "                  'tag': 'NNP',\n",
      "                  'word': 'Basuki'},\n",
      "             21: {'address': 21,\n",
      "                  'ctag': 'NNP',\n",
      "                  'deps': defaultdict(<class 'list'>, {'case': [22]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 24,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'nmod:poss',\n",
      "                  'tag': 'NNP',\n",
      "                  'word': 'Ahok'},\n",
      "             22: {'address': 22,\n",
      "                  'ctag': 'POS',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 21,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'case',\n",
      "                  'tag': 'POS',\n",
      "                  'word': \"'\"},\n",
      "             23: {'address': 23,\n",
      "                  'ctag': 'NNP',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 24,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'compound',\n",
      "                  'tag': 'NNP',\n",
      "                  'word': 'Tjahaja'},\n",
      "             24: {'address': 24,\n",
      "                  'ctag': 'NNP',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'case': [25],\n",
      "                                       'compound': [23],\n",
      "                                       'nmod:poss': [21]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 26,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'nmod:poss',\n",
      "                  'tag': 'NNP',\n",
      "                  'word': 'Purnama'},\n",
      "             25: {'address': 25,\n",
      "                  'ctag': 'POS',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 24,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'case',\n",
      "                  'tag': 'POS',\n",
      "                  'word': \"'s\"},\n",
      "             26: {'address': 26,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>, {'nmod:poss': [24]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 27,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'nsubj',\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'letter'},\n",
      "             27: {'address': 27,\n",
      "                  'ctag': 'VBG',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'dobj': [29],\n",
      "                                       'nsubj': [26]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 16,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'advcl',\n",
      "                  'tag': 'VBG',\n",
      "                  'word': 'recommending'},\n",
      "             28: {'address': 28,\n",
      "                  'ctag': 'DT',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 29,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'det',\n",
      "                  'tag': 'DT',\n",
      "                  'word': 'the'},\n",
      "             29: {'address': 29,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'det': [28],\n",
      "                                       'nmod': [32]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 27,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'dobj',\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'disbandment'},\n",
      "             30: {'address': 30,\n",
      "                  'ctag': 'IN',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 32,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'case',\n",
      "                  'tag': 'IN',\n",
      "                  'word': 'of'},\n",
      "             31: {'address': 31,\n",
      "                  'ctag': 'JJ',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 32,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'amod',\n",
      "                  'tag': 'JJ',\n",
      "                  'word': 'hard-line'},\n",
      "             32: {'address': 32,\n",
      "                  'ctag': 'NN',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'amod': [31],\n",
      "                                       'case': [30]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 29,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'nmod',\n",
      "                  'tag': 'NN',\n",
      "                  'word': 'organization'},\n",
      "             33: {'address': 33,\n",
      "                  'ctag': 'DT',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 36,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'det',\n",
      "                  'tag': 'DT',\n",
      "                  'word': 'the'},\n",
      "             34: {'address': 34,\n",
      "                  'ctag': 'NNP',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 36,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'compound',\n",
      "                  'tag': 'NNP',\n",
      "                  'word': 'Islam'},\n",
      "             35: {'address': 35,\n",
      "                  'ctag': 'NNP',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 36,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'compound',\n",
      "                  'tag': 'NNP',\n",
      "                  'word': 'Defenders'},\n",
      "             36: {'address': 36,\n",
      "                  'ctag': 'NNP',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'appos': [38],\n",
      "                                       'compound': [34, 35],\n",
      "                                       'det': [33]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 6,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'nsubj',\n",
      "                  'tag': 'NNP',\n",
      "                  'word': 'Front'},\n",
      "             38: {'address': 38,\n",
      "                  'ctag': 'NNP',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 36,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'appos',\n",
      "                  'tag': 'NNP',\n",
      "                  'word': 'FPI'}})\n"
     ]
    }
   ],
   "source": [
    "depParse = list(stanford.depParser.raw_parse_sents(article)) #Converting the iterator to a list so we can call by index. They are still \n",
    "firstSentDepParseTree = list(depParse[0])[0] #iterators so be careful about re-running code, without re-running this block\n",
    "print(firstSentDepParseTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"1022pt\" height=\"818pt\"\n",
       " viewBox=\"0.00 0.00 1021.70 818.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 814)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-814 1017.6968,-814 1017.6968,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"357.019\" y=\"-787.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"357.019\" y=\"-701.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">6 (said)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;6 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M357.019,-773.7616C357.019,-762.3597 357.019,-747.4342 357.019,-734.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"360.5191,-734.2121 357.019,-724.2121 353.5191,-734.2121 360.5191,-734.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"368.2949\" y=\"-744.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"229.019\" y=\"-615.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">5 (has)</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;5 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>6&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M330.186,-687.9716C311.071,-675.1286 285.1866,-657.7376 264.3856,-643.7619\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"266.0692,-640.6764 255.8167,-638.0047 262.1653,-646.4868 266.0692,-640.6764\"/>\n",
       "<text text-anchor=\"middle\" x=\"311.126\" y=\"-658.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dep</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>11</title>\n",
       "<text text-anchor=\"middle\" x=\"357.019\" y=\"-615.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">11 (ready)</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;11 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>6&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M357.019,-687.7616C357.019,-676.3597 357.019,-661.4342 357.019,-648.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"360.5191,-648.2121 357.019,-638.2121 353.5191,-648.2121 360.5191,-648.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"375.6777\" y=\"-658.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ccomp</text>\n",
       "</g>\n",
       "<!-- 36 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>36</title>\n",
       "<text text-anchor=\"middle\" x=\"650.019\" y=\"-615.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">36 (Front)</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;36 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>6&#45;&gt;36</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M386.4545,-697.3602C437.4544,-682.391 542.6181,-651.5238 603.5616,-633.636\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"604.7022,-636.9489 613.3117,-630.7742 602.7308,-630.2322 604.7022,-636.9489\"/>\n",
       "<text text-anchor=\"middle\" x=\"537.188\" y=\"-658.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"35.019\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (Home)</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"130.019\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (Minister)</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"226.019\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3 (Tjahjo)</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"131.019\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (Kumolo)</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>4&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M92.6548,-515.9476C83.9721,-510.8571 75.1813,-504.8186 67.9155,-498 60.869,-491.3872 54.6007,-482.9804 49.4752,-474.9557\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"52.3469,-472.9394 44.1934,-466.1739 46.3483,-476.5473 52.3469,-472.9394\"/>\n",
       "<text text-anchor=\"middle\" x=\"97.5708\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>4&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M130.807,-515.7616C130.6744,-504.3597 130.5008,-489.4342 130.3504,-476.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"133.8469,-476.1707 130.2308,-466.2121 126.8474,-476.2522 133.8469,-476.1707\"/>\n",
       "<text text-anchor=\"middle\" x=\"160.5708\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;3 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>4&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M169.7106,-515.8108C178.2692,-510.7587 186.899,-504.7715 194.019,-498 200.9643,-491.3947 207.0865,-482.9899 212.0671,-474.9648\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"215.1748,-476.5834 217.1883,-466.1816 209.1277,-473.0575 215.1748,-476.5834\"/>\n",
       "<text text-anchor=\"middle\" x=\"235.5708\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;4 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>5&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M201.6821,-607.8789C189.8054,-601.7895 176.2287,-593.645 165.6812,-584 158.354,-577.2998 151.7344,-568.7788 146.2906,-560.6774\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"149.1896,-558.7137 140.8671,-552.1573 143.2844,-562.4727 149.1896,-558.7137\"/>\n",
       "<text text-anchor=\"middle\" x=\"181.188\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"218.019\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">7 (that)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;7 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>11&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M325.9845,-601.887C316.4102,-596.2259 305.8604,-589.9134 296.2534,-584 282.5347,-575.5557 267.6044,-566.0846 254.4991,-557.6752\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"256.0584,-554.5166 245.755,-552.0485 252.2705,-560.4032 256.0584,-554.5166\"/>\n",
       "<text text-anchor=\"middle\" x=\"311.4019\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"306.019\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">9 (ministry)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;9 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>11&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M346.2033,-601.7616C339.1712,-589.9036 329.879,-574.2345 322.0016,-560.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"324.9305,-559.0281 316.8192,-552.2121 318.9096,-562.5987 324.9305,-559.0281\"/>\n",
       "<text text-anchor=\"middle\" x=\"350.188\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"392.019\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">10 (is)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;10 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>11&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M364.4416,-601.7616C369.1748,-590.1316 375.3998,-574.8357 380.7376,-561.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"384.0793,-562.7938 384.6071,-552.2121 377.5957,-560.1551 384.0793,-562.7938\"/>\n",
       "<text text-anchor=\"middle\" x=\"387.126\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cop</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<text text-anchor=\"middle\" x=\"477.019\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">13 (follow)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>11&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M382.175,-601.9716C400.0153,-589.186 424.1454,-571.8928 443.6014,-557.9493\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"445.8069,-560.6748 451.8962,-552.0047 441.7292,-554.985 445.8069,-560.6748\"/>\n",
       "<text text-anchor=\"middle\" x=\"444.0708\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">xcomp</text>\n",
       "</g>\n",
       "<!-- 33 -->\n",
       "<g id=\"node34\" class=\"node\">\n",
       "<title>33</title>\n",
       "<text text-anchor=\"middle\" x=\"565.019\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">33 (the)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;33 -->\n",
       "<g id=\"edge33\" class=\"edge\">\n",
       "<title>36&#45;&gt;33</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M631.9927,-601.7616C619.8218,-589.4475 603.5888,-573.0235 590.1526,-559.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"592.5383,-556.8641 583.0194,-552.2121 587.5597,-561.7848 592.5383,-556.8641\"/>\n",
       "<text text-anchor=\"middle\" x=\"621.5708\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 34 -->\n",
       "<g id=\"node35\" class=\"node\">\n",
       "<title>34</title>\n",
       "<text text-anchor=\"middle\" x=\"650.019\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">34 (Islam)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;34 -->\n",
       "<g id=\"edge34\" class=\"edge\">\n",
       "<title>36&#45;&gt;34</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M650.019,-601.7616C650.019,-590.3597 650.019,-575.4342 650.019,-562.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"653.5191,-562.2121 650.019,-552.2121 646.5191,-562.2121 653.5191,-562.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"679.5708\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 35 -->\n",
       "<g id=\"node36\" class=\"node\">\n",
       "<title>35</title>\n",
       "<text text-anchor=\"middle\" x=\"755.019\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">35 (Defenders)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;35 -->\n",
       "<g id=\"edge35\" class=\"edge\">\n",
       "<title>36&#45;&gt;35</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M686.7787,-601.8896C695.776,-596.6928 705.0895,-590.6201 713.019,-584 721.3735,-577.0252 729.3683,-568.2502 736.0984,-560.0061\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"738.944,-562.0496 742.3735,-552.0252 733.4412,-557.723 738.944,-562.0496\"/>\n",
       "<text text-anchor=\"middle\" x=\"757.5708\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 38 -->\n",
       "<g id=\"node37\" class=\"node\">\n",
       "<title>38</title>\n",
       "<text text-anchor=\"middle\" x=\"855.019\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">38 (FPI)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;38 -->\n",
       "<g id=\"edge36\" class=\"edge\">\n",
       "<title>36&#45;&gt;38</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M686.8076,-614.8838C716.1413,-609.8138 757.6848,-600.3741 791.019,-584 804.3311,-577.461 817.5087,-567.8541 828.4149,-558.8044\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"830.9706,-561.2232 836.2767,-552.0528 826.41,-555.9127 830.9706,-561.2232\"/>\n",
       "<text text-anchor=\"middle\" x=\"829.3501\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">appos</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"307.019\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">8 (his)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;8 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>9&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M306.2311,-515.7616C306.3637,-504.3597 306.5373,-489.4342 306.6877,-476.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"310.1907,-476.2522 306.8073,-466.2121 303.1911,-476.1707 310.1907,-476.2522\"/>\n",
       "<text text-anchor=\"middle\" x=\"337.3569\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod:poss</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"404.019\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">12 (to)</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;12 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>13&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M460.1322,-515.8785C454.9634,-510.217 449.3096,-503.9064 444.2534,-498 437.7617,-490.4166 430.8858,-482.0171 424.6898,-474.3015\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"427.2551,-471.9047 418.2837,-466.2687 421.7823,-476.2692 427.2551,-471.9047\"/>\n",
       "<text text-anchor=\"middle\" x=\"459.4019\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>14</title>\n",
       "<text text-anchor=\"middle\" x=\"477.019\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">14 (up)</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;14 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>13&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M477.019,-515.7616C477.019,-504.3597 477.019,-489.4342 477.019,-476.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"480.5191,-476.2121 477.019,-466.2121 473.5191,-476.2121 480.5191,-476.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"516.2915\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound:prt</text>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>16</title>\n",
       "<text text-anchor=\"middle\" x=\"579.019\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">16 (acting)</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;16 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>13&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M517.0467,-519.1038C533.7899,-512.2519 550.7989,-504.2723 557.019,-498 563.1909,-491.7764 567.7738,-483.5296 571.1107,-475.5317\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"574.43,-476.6458 574.5859,-466.0521 567.8577,-474.2363 574.43,-476.6458\"/>\n",
       "<text text-anchor=\"middle\" x=\"582.1777\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advcl</text>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>15</title>\n",
       "<text text-anchor=\"middle\" x=\"492.019\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">15 (on)</text>\n",
       "</g>\n",
       "<!-- 16&#45;&gt;15 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>16&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M560.5686,-429.7616C548.1113,-417.4475 531.4963,-401.0235 517.744,-387.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"520.0153,-384.7531 510.4429,-380.2121 515.0942,-389.7314 520.0153,-384.7531\"/>\n",
       "<text text-anchor=\"middle\" x=\"555.4019\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>19</title>\n",
       "<text text-anchor=\"middle\" x=\"579.019\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">19 (Basuki)</text>\n",
       "</g>\n",
       "<!-- 16&#45;&gt;19 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>16&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M579.019,-429.7616C579.019,-418.3597 579.019,-403.4342 579.019,-390.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"582.5191,-390.2121 579.019,-380.2121 575.5191,-390.2121 582.5191,-390.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"591.4639\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dobj</text>\n",
       "</g>\n",
       "<!-- 27 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>27</title>\n",
       "<text text-anchor=\"middle\" x=\"701.019\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">27 (recommending)</text>\n",
       "</g>\n",
       "<!-- 16&#45;&gt;27 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>16&#45;&gt;27</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M604.5942,-429.9716C622.732,-417.186 647.2642,-399.8928 667.0444,-385.9493\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"669.3207,-388.627 675.4775,-380.0047 665.2875,-382.9056 669.3207,-388.627\"/>\n",
       "<text text-anchor=\"middle\" x=\"663.1777\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advcl</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>17</title>\n",
       "<text text-anchor=\"middle\" x=\"476.019\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">17 (Jakarta)</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;17 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>19&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M543.7022,-343.9494C534.877,-338.7127 525.7127,-332.606 517.9155,-326 509.7499,-319.0819 501.9038,-310.458 495.254,-302.3304\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"497.9777,-300.132 489.0354,-294.4499 492.4825,-304.4683 497.9777,-300.132\"/>\n",
       "<text text-anchor=\"middle\" x=\"547.5708\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>18</title>\n",
       "<text text-anchor=\"middle\" x=\"582.019\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">18 (governor)</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;18 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>19&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M579.6553,-343.7616C580.053,-332.3597 580.5737,-317.4342 581.0251,-304.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"584.5329,-304.3281 581.3837,-294.2121 577.5371,-304.084 584.5329,-304.3281\"/>\n",
       "<text text-anchor=\"middle\" x=\"610.5708\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>26</title>\n",
       "<text text-anchor=\"middle\" x=\"693.019\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">26 (letter)</text>\n",
       "</g>\n",
       "<!-- 27&#45;&gt;26 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>27&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M699.3224,-343.7616C698.2618,-332.3597 696.8734,-317.4342 695.6696,-304.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"699.1245,-303.8449 694.7132,-294.2121 692.1546,-304.4933 699.1245,-303.8449\"/>\n",
       "<text text-anchor=\"middle\" x=\"713.188\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>29</title>\n",
       "<text text-anchor=\"middle\" x=\"804.019\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">29 (disbandment)</text>\n",
       "</g>\n",
       "<!-- 27&#45;&gt;29 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>27&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M722.8627,-343.7616C737.8841,-331.2195 758.0118,-314.4138 774.464,-300.677\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"776.774,-303.3079 782.2069,-294.2121 772.2875,-297.9347 776.774,-303.3079\"/>\n",
       "<text text-anchor=\"middle\" x=\"771.4639\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dobj</text>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>21</title>\n",
       "<text text-anchor=\"middle\" x=\"596.019\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">21 (Ahok)</text>\n",
       "</g>\n",
       "<!-- 22 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>22</title>\n",
       "<text text-anchor=\"middle\" x=\"596.019\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">22 (&#39;)</text>\n",
       "</g>\n",
       "<!-- 21&#45;&gt;22 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>21&#45;&gt;22</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M596.019,-85.7616C596.019,-74.3597 596.019,-59.4342 596.019,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"599.5191,-46.2121 596.019,-36.2121 592.5191,-46.2121 599.5191,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"608.064\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>23</title>\n",
       "<text text-anchor=\"middle\" x=\"693.019\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">23 (Tjahaja)</text>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>24</title>\n",
       "<text text-anchor=\"middle\" x=\"693.019\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">24 (Purnama)</text>\n",
       "</g>\n",
       "<!-- 24&#45;&gt;21 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>24&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M653.3103,-171.9247C644.5235,-166.8679 635.6609,-160.8501 628.3433,-154 621.3253,-147.4305 615.1404,-139.0355 610.1095,-131.0083\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"613.0251,-129.0618 604.9368,-122.2186 606.9922,-132.6121 613.0251,-129.0618\"/>\n",
       "<text text-anchor=\"middle\" x=\"659.3569\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod:poss</text>\n",
       "</g>\n",
       "<!-- 24&#45;&gt;23 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>24&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M693.019,-171.7616C693.019,-160.3597 693.019,-145.4342 693.019,-132.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"696.5191,-132.2121 693.019,-122.2121 689.5191,-132.2121 696.5191,-132.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"722.5708\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>25</title>\n",
       "<text text-anchor=\"middle\" x=\"780.019\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">25 (&#39;s)</text>\n",
       "</g>\n",
       "<!-- 24&#45;&gt;25 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>24&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M733.3116,-171.8305C741.5091,-166.869 749.5818,-160.9156 756.019,-154 761.989,-147.5863 766.7473,-139.4145 770.401,-131.5448\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"773.6663,-132.8088 774.317,-122.2339 767.2138,-130.0949 773.6663,-132.8088\"/>\n",
       "<text text-anchor=\"middle\" x=\"778.064\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;24 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>26&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M693.019,-257.7616C693.019,-246.3597 693.019,-231.4342 693.019,-218.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"696.5191,-218.2121 693.019,-208.2121 689.5191,-218.2121 696.5191,-218.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"723.3569\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod:poss</text>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>28</title>\n",
       "<text text-anchor=\"middle\" x=\"795.019\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">28 (the)</text>\n",
       "</g>\n",
       "<!-- 29&#45;&gt;28 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>29&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M802.1104,-257.7616C800.9171,-246.3597 799.3552,-231.4342 798.001,-218.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"801.4469,-217.7935 796.925,-208.2121 794.4849,-218.5221 801.4469,-217.7935\"/>\n",
       "<text text-anchor=\"middle\" x=\"808.5708\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 32 -->\n",
       "<g id=\"node31\" class=\"node\">\n",
       "<title>32</title>\n",
       "<text text-anchor=\"middle\" x=\"899.019\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">32 (organization)</text>\n",
       "</g>\n",
       "<!-- 29&#45;&gt;32 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>29&#45;&gt;32</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M824.1661,-257.7616C837.8948,-245.3335 856.248,-228.719 871.3449,-215.0524\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"873.8364,-217.518 878.901,-208.2121 869.1386,-212.3286 873.8364,-217.518\"/>\n",
       "<text text-anchor=\"middle\" x=\"872.9639\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 30 -->\n",
       "<g id=\"node32\" class=\"node\">\n",
       "<title>30</title>\n",
       "<text text-anchor=\"middle\" x=\"875.019\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">30 (of)</text>\n",
       "</g>\n",
       "<!-- 32&#45;&gt;30 -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>32&#45;&gt;30</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M893.9293,-171.7616C890.7155,-160.2456 886.4987,-145.1353 882.8627,-132.1064\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"886.1607,-130.9032 880.1015,-122.2121 879.4184,-132.7849 886.1607,-130.9032\"/>\n",
       "<text text-anchor=\"middle\" x=\"900.064\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 31 -->\n",
       "<g id=\"node33\" class=\"node\">\n",
       "<title>31</title>\n",
       "<text text-anchor=\"middle\" x=\"967.019\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">31 (hard&#45;line)</text>\n",
       "</g>\n",
       "<!-- 32&#45;&gt;31 -->\n",
       "<g id=\"edge32\" class=\"edge\">\n",
       "<title>32&#45;&gt;31</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M913.4401,-171.7616C922.9965,-159.6756 935.6834,-143.6304 946.3127,-130.1874\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"949.1618,-132.2271 952.6188,-122.2121 943.6709,-127.8854 949.1618,-132.2271\"/>\n",
       "<text text-anchor=\"middle\" x=\"952.5708\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x110d50f28>"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    firstSentGraph = graphviz.Source(firstSentDepParseTree.to_dot())\n",
    "except:\n",
    "    firstSentGraph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "firstSentGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information extraction\n",
    "\n",
    "Information extraction approaches typically (as here, with Stanford's Open IE engine) ride atop the dependency parse of a sentence. They are a pre-coded example of the type analyzed in the prior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OpenIE run\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.2 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 16.946 (s)\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [18.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator natlog\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie\n",
      "[main] INFO edu.stanford.nlp.naturalli.ClauseSplitter - Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.0108 seconds]\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - Processing file: /var/folders/7j/8jdgq3nd7_b1h51vzjsqgfhw0000gn/T/tmpglebve9e\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - All files have been queued; awaiting termination...\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - DONE processing files. 0 exceptions encountered.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ieDF = stanford.openIE(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`openIE()` prints everything stanford core produces and we can see from looking at it that initializing the dependency parser takes most of the time, so calling the function will always take at least 12 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certainty</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>elephant</td>\n",
       "      <td>is in</td>\n",
       "      <td>my pajamas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I</td>\n",
       "      <td>saw</td>\n",
       "      <td>elephant in my pajamas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I</td>\n",
       "      <td>saw</td>\n",
       "      <td>elephant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in interview with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in recent interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed stimulus efforts in</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in recent intervie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in recent interview with Wall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in recent interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>short-term stimulus efforts</td>\n",
       "      <td>is in</td>\n",
       "      <td>recent interview with Wall Street Journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in interview with Wall Street...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>recent interview</td>\n",
       "      <td>is with</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Martin</td>\n",
       "      <td>was</td>\n",
       "      <td>African</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was African American from</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was American from</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was</td>\n",
       "      <td>American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was</td>\n",
       "      <td>African American</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    certainty                      subject                           verb  \\\n",
       "0         1.0                     elephant                          is in   \n",
       "1         1.0                            I                            saw   \n",
       "2         1.0                            I                            saw   \n",
       "3         1.0              quick brown fox                    jumped over   \n",
       "4         1.0              quick brown fox                    jumped over   \n",
       "5         1.0                    quick fox                    jumped over   \n",
       "6         1.0                          fox                    jumped over   \n",
       "7         1.0                    brown fox                    jumped over   \n",
       "8         1.0                    brown fox                    jumped over   \n",
       "9         1.0                    quick fox                    jumped over   \n",
       "10        1.0                          fox                    jumped over   \n",
       "11        1.0            Christine Lagarde                      discussed   \n",
       "12        1.0            Christine Lagarde                      discussed   \n",
       "13        1.0            Christine Lagarde                      discussed   \n",
       "14        1.0            Christine Lagarde  discussed stimulus efforts in   \n",
       "15        1.0            Christine Lagarde                      discussed   \n",
       "16        1.0            Christine Lagarde                      discussed   \n",
       "17        1.0            Christine Lagarde                      discussed   \n",
       "18        1.0  short-term stimulus efforts                          is in   \n",
       "19        1.0            Christine Lagarde                      discussed   \n",
       "20        1.0            Christine Lagarde                      discussed   \n",
       "21        1.0            Christine Lagarde                      discussed   \n",
       "22        1.0            Christine Lagarde                      discussed   \n",
       "23        1.0             recent interview                        is with   \n",
       "24        1.0                       Martin                            was   \n",
       "25        1.0      Trayvon Benjamin Martin      was African American from   \n",
       "26        1.0      Trayvon Benjamin Martin              was American from   \n",
       "27        1.0      Trayvon Benjamin Martin                            was   \n",
       "28        1.0      Trayvon Benjamin Martin                            was   \n",
       "\n",
       "                                               object  \n",
       "0                                          my pajamas  \n",
       "1                              elephant in my pajamas  \n",
       "2                                            elephant  \n",
       "3                                            lazy dog  \n",
       "4                                                 dog  \n",
       "5                                                 dog  \n",
       "6                                                 dog  \n",
       "7                                            lazy dog  \n",
       "8                                                 dog  \n",
       "9                                            lazy dog  \n",
       "10                                           lazy dog  \n",
       "11  short-term stimulus efforts in interview with ...  \n",
       "12                      stimulus efforts in interview  \n",
       "13               stimulus efforts in recent interview  \n",
       "14                                             France  \n",
       "15  short-term stimulus efforts in recent intervie...  \n",
       "16  stimulus efforts in recent interview with Wall...  \n",
       "17    short-term stimulus efforts in recent interview  \n",
       "18          recent interview with Wall Street Journal  \n",
       "19  stimulus efforts in interview with Wall Street...  \n",
       "20                                   stimulus efforts  \n",
       "21                        short-term stimulus efforts  \n",
       "22           short-term stimulus efforts in interview  \n",
       "23                                Wall Street Journal  \n",
       "24                                            African  \n",
       "25                                            Florida  \n",
       "26                                            Florida  \n",
       "27                                           American  \n",
       "28                                   African American  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No buffalos (because there were no verbs), but the rest is somewhat promising. Note, however, that it abandoned the key theme of the sentence about the tragic Trayvon Martin death (\"fatally shot\"), likely because it was buried so deeply within the complex phrase structure. This is obviously a challenge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 4*</span>\n",
    "\n",
    "<span style=\"color:red\">How would you extract relevant information about the Trayvon Martin sentence directly from the dependency parse (above)? Code an example here. (For instance, what compound nouns show up with what verb phrases within the sentence?) How could these approaches inform your research project?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How could these approaches inform your research project?**<br>\n",
    "<i>The analysis could show the social game in which social actors. For example, the way I did it was that 1) I identified the subjects mentioned in the text by coding:\n",
    "\n",
    "    ie_DF['subject'].value_counts()\n",
    "\n",
    "to look at the mot frequent subjects. 2) Then I looked at subjects that I am interested in and look at verbs that are related to them by coding:\n",
    "\n",
    "    ie_DF[ie_DF['subject'] == 'demonstrators']['verb'].value_counts()\n",
    "\n",
    "and 3) finally inferring the social game by looking at the subject and the verb.</i>\n",
    "\n",
    "**<i>(!) For complete codes and explanation, please look at Exercise 5</i>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ieDF[ieDF['subject'] == 'I']['verb'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also look for subject, object, target triples in one of the reddit stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OpenIE run\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.2 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 16.627 (s)\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [18.3 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator natlog\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie\n",
      "[main] INFO edu.stanford.nlp.naturalli.ClauseSplitter - Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.0101 seconds]\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - Processing file: /var/folders/7j/8jdgq3nd7_b1h51vzjsqgfhw0000gn/T/tmpqq50q8xd\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - All files have been queued; awaiting termination...\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - DONE processing files. 0 exceptions encountered.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ieDF = stanford.openIE(redditTopScores['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certainty</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>'ll get</td>\n",
       "      <td>calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>Quite often 'll get</td>\n",
       "      <td>calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>often 'll get</td>\n",
       "      <td>calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.831036</td>\n",
       "      <td>we</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>straight analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>straight analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>would supply analog cable to</td>\n",
       "      <td>homes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.831036</td>\n",
       "      <td>we</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.831036</td>\n",
       "      <td>we</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>straight analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>straight analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.831036</td>\n",
       "      <td>we</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>would supply analog cable to</td>\n",
       "      <td>many homes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>our equipment</td>\n",
       "      <td>receive</td>\n",
       "      <td>channels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>our digital equipment</td>\n",
       "      <td>receive</td>\n",
       "      <td>channels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>repeat offenders</td>\n",
       "      <td>is with</td>\n",
       "      <td>long ticket histories of types of issues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>anyway get</td>\n",
       "      <td>call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>get</td>\n",
       "      <td>call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>he</td>\n",
       "      <td>speak with</td>\n",
       "      <td>machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>So anyway get</td>\n",
       "      <td>call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>he</td>\n",
       "      <td>has</td>\n",
       "      <td>speak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>he</td>\n",
       "      <td>has</td>\n",
       "      <td>speak with machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>call</td>\n",
       "      <td>however was going</td>\n",
       "      <td>little different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>call</td>\n",
       "      <td>was going</td>\n",
       "      <td>little different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.780294</td>\n",
       "      <td>handling</td>\n",
       "      <td>types of</td>\n",
       "      <td>customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>call</td>\n",
       "      <td>was going</td>\n",
       "      <td>different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>man happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>man happy again for once in long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>old man happy for once in time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>man</td>\n",
       "      <td>happy for</td>\n",
       "      <td>once</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>old man</td>\n",
       "      <td>happy again for</td>\n",
       "      <td>once in time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>man</td>\n",
       "      <td>happy again for</td>\n",
       "      <td>once</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>old man happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>old man</td>\n",
       "      <td>happy again for</td>\n",
       "      <td>once in very long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>man happy again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>man</td>\n",
       "      <td>happy for</td>\n",
       "      <td>once in long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>old man happy again for once in time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>man happy again for once</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>old man happy again for once in very long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>old man happy again for once in long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>old man</td>\n",
       "      <td>happy for</td>\n",
       "      <td>once in time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>man happy for once in time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>man happy for once in very long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>old man happy for once in very long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>man happy again for once in long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>man happy again for once in very long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>letter</td>\n",
       "      <td>put on</td>\n",
       "      <td>our front entrance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>letter</td>\n",
       "      <td>put to</td>\n",
       "      <td>retail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>letter</td>\n",
       "      <td>put on</td>\n",
       "      <td>our entrance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>letter</td>\n",
       "      <td>was</td>\n",
       "      <td>framed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>they</td>\n",
       "      <td>going through</td>\n",
       "      <td>lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>they</td>\n",
       "      <td>just going through</td>\n",
       "      <td>lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>still think about</td>\n",
       "      <td>Mr. Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>still think occasionally about</td>\n",
       "      <td>Mr. Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>think occasionally about</td>\n",
       "      <td>Mr. Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>think about</td>\n",
       "      <td>Mr. Smith</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     certainty                subject                            verb  \\\n",
       "0     1.000000                     we                         'll get   \n",
       "1     1.000000                     we             Quite often 'll get   \n",
       "2     1.000000                     we                   often 'll get   \n",
       "3     0.831036                     we                            coax   \n",
       "4     0.774359  straight analog cable                            coax   \n",
       "5     0.774359           analog cable                            coax   \n",
       "6     0.774359  straight analog cable                            coax   \n",
       "7     1.000000                     we    would supply analog cable to   \n",
       "8     0.831036                     we                            coax   \n",
       "9     0.774359           analog cable                            coax   \n",
       "10    0.831036                     we                            coax   \n",
       "11    0.774359  straight analog cable                            coax   \n",
       "12    0.774359  straight analog cable                            coax   \n",
       "13    0.831036                     we                            coax   \n",
       "14    0.774359           analog cable                            coax   \n",
       "15    1.000000                     we    would supply analog cable to   \n",
       "16    0.774359           analog cable                            coax   \n",
       "17    1.000000          our equipment                         receive   \n",
       "18    1.000000  our digital equipment                         receive   \n",
       "19    1.000000       repeat offenders                         is with   \n",
       "20    1.000000                      I                      anyway get   \n",
       "21    1.000000                      I                             get   \n",
       "22    1.000000                     he                      speak with   \n",
       "23    1.000000                      I                   So anyway get   \n",
       "24    1.000000                     he                             has   \n",
       "25    1.000000                     he                             has   \n",
       "26    1.000000                   call               however was going   \n",
       "27    1.000000                   call                       was going   \n",
       "28    0.780294               handling                        types of   \n",
       "29    1.000000                   call                       was going   \n",
       "..         ...                    ...                             ...   \n",
       "161   1.000000                     it                            made   \n",
       "162   1.000000                     it                            made   \n",
       "163   1.000000                     it                     really made   \n",
       "164   1.000000                    man                       happy for   \n",
       "165   1.000000                old man                 happy again for   \n",
       "166   1.000000                    man                 happy again for   \n",
       "167   1.000000                     it                     really made   \n",
       "168   1.000000                old man                 happy again for   \n",
       "169   1.000000                     it                     really made   \n",
       "170   1.000000                    man                       happy for   \n",
       "171   1.000000                     it                            made   \n",
       "172   1.000000                     it                            made   \n",
       "173   1.000000                     it                     really made   \n",
       "174   1.000000                     it                     really made   \n",
       "175   1.000000                old man                       happy for   \n",
       "176   1.000000                     it                     really made   \n",
       "177   1.000000                     it                            made   \n",
       "178   1.000000                     it                            made   \n",
       "179   1.000000                     it                     really made   \n",
       "180   1.000000                     it                            made   \n",
       "181   1.000000                 letter                          put on   \n",
       "182   1.000000                 letter                          put to   \n",
       "183   1.000000                 letter                          put on   \n",
       "184   1.000000                 letter                             was   \n",
       "185   1.000000                   they                   going through   \n",
       "186   1.000000                   they              just going through   \n",
       "187   1.000000                      I               still think about   \n",
       "188   1.000000                      I  still think occasionally about   \n",
       "189   1.000000                      I        think occasionally about   \n",
       "190   1.000000                      I                     think about   \n",
       "\n",
       "                                             object  \n",
       "0                                             calls  \n",
       "1                                             calls  \n",
       "2                                             calls  \n",
       "3                                      direct to TV  \n",
       "4                                  direct from wall  \n",
       "5                            direct from wall to TV  \n",
       "6                                      direct to TV  \n",
       "7                                             homes  \n",
       "8                                  direct from wall  \n",
       "9                                  direct from wall  \n",
       "10                           direct from wall to TV  \n",
       "11                           direct from wall to TV  \n",
       "12                                           direct  \n",
       "13                                           direct  \n",
       "14                                     direct to TV  \n",
       "15                                       many homes  \n",
       "16                                           direct  \n",
       "17                                         channels  \n",
       "18                                         channels  \n",
       "19         long ticket histories of types of issues  \n",
       "20                                             call  \n",
       "21                                             call  \n",
       "22                                          machine  \n",
       "23                                             call  \n",
       "24                                            speak  \n",
       "25                               speak with machine  \n",
       "26                                 little different  \n",
       "27                                 little different  \n",
       "28                                        customers  \n",
       "29                                        different  \n",
       "..                                              ...  \n",
       "161                                       man happy  \n",
       "162           man happy again for once in long time  \n",
       "163                  old man happy for once in time  \n",
       "164                                            once  \n",
       "165                                    once in time  \n",
       "166                                            once  \n",
       "167                                   old man happy  \n",
       "168                          once in very long time  \n",
       "169                                 man happy again  \n",
       "170                               once in long time  \n",
       "171            old man happy again for once in time  \n",
       "172                        man happy again for once  \n",
       "173  old man happy again for once in very long time  \n",
       "174       old man happy again for once in long time  \n",
       "175                                    once in time  \n",
       "176                      man happy for once in time  \n",
       "177            man happy for once in very long time  \n",
       "178        old man happy for once in very long time  \n",
       "179           man happy again for once in long time  \n",
       "180      man happy again for once in very long time  \n",
       "181                              our front entrance  \n",
       "182                                          retail  \n",
       "183                                    our entrance  \n",
       "184                                          framed  \n",
       "185                                             lot  \n",
       "186                                             lot  \n",
       "187                                       Mr. Smith  \n",
       "188                                       Mr. Smith  \n",
       "189                                       Mr. Smith  \n",
       "190                                       Mr. Smith  \n",
       "\n",
       "[191 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's almost 200 triples in only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(redditTopScores['sentences'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentences and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "971"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(s) for s in redditTopScores['sentences'][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find at the most common subject in this story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I                        48\n",
       "it                       42\n",
       "he                       19\n",
       "He                       18\n",
       "we                       11\n",
       "old man                   8\n",
       "man                       8\n",
       "letter                    4\n",
       "straight analog cable     4\n",
       "call                      4\n",
       "our booking calendar      4\n",
       "analog cable              4\n",
       "my supervisor             3\n",
       "you                       2\n",
       "TV                        2\n",
       "his TV set                2\n",
       "they                      2\n",
       "our equipment             1\n",
       "me                        1\n",
       "handling                  1\n",
       "people                    1\n",
       "our digital equipment     1\n",
       "repeat offenders          1\n",
       "Name: subject, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I is followed by various male pronouns and compound nouns (e.g., \"old man\"). 'I' occures most often with the following verbs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "could come                        8\n",
       "even brought                      5\n",
       "brought                           5\n",
       "was                               4\n",
       "had                               4\n",
       "speak for                         3\n",
       "instantly felt                    1\n",
       "get to                            1\n",
       "still think about                 1\n",
       "felt                              1\n",
       "think about                       1\n",
       "anyway get                        1\n",
       "complaint in                      1\n",
       "took                              1\n",
       "get                               1\n",
       "'ve dealt with                    1\n",
       "eventually had                    1\n",
       "had cable within                  1\n",
       "have                              1\n",
       "think occasionally about          1\n",
       "do                                1\n",
       "still think occasionally about    1\n",
       "speak with                        1\n",
       "ask                               1\n",
       "So anyway get                     1\n",
       "Name: verb, dtype: int64"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF[ieDF['subject'] == 'I']['verb'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the following objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mr. Smith                                             4\n",
       "him                                                   3\n",
       "call                                                  3\n",
       "simplified remote                                     2\n",
       "simplified remote for his set top box                 2\n",
       "remote for his set top box                            2\n",
       "get                                                   2\n",
       "this                                                  2\n",
       "bad                                                   2\n",
       "remote                                                2\n",
       "willing                                               2\n",
       "speak for bit about account for Mr. Smith             1\n",
       "cable running again                                   1\n",
       "residence                                             1\n",
       "speak with her for bit about account                  1\n",
       "experience                                            1\n",
       "bit                                                   1\n",
       "it                                                    1\n",
       "speak with her                                        1\n",
       "cable running                                         1\n",
       "bit about account                                     1\n",
       "how useless                                           1\n",
       "her                                                   1\n",
       "cable                                                 1\n",
       "useless                                               1\n",
       "bit about account for Mr. Smith                       1\n",
       "speak                                                 1\n",
       "speak for bit                                         1\n",
       "speak with her for bit                                1\n",
       "speak for bit about account                           1\n",
       "speak with her for bit about account for Mr. Smith    1\n",
       "book                                                  1\n",
       "30 seconds                                            1\n",
       "Name: object, dtype: int64"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF[ieDF['subject'] == 'I']['object'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run the corenlp server. When you run this server (with the command below), you can click on the browswer link provided to experiment with it. Note that when we run the server, executing the command below, it interrupts the current jupyter process and you will not be able to run code here again (processes will \"hang\" and never finish) until you interrup the process by clicking \"Kernel\" and then \"Interrupt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server on http://localhost:16432 , please wait a few seconds\n",
      "click Kernel -> Then Interupt to stop     (･ω･｀)))                    \n",
      "Exiting (ノ≧▽≦)ノ\n"
     ]
    }
   ],
   "source": [
    "stanford.startCoreServer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"color:red\">*Exercise 5*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform open information extraction on a modest subset of texts relevant to your final project. Analyze the relative attachment of several subjects relative to verbs and objects and visa versa. Describe how you would select among these statements to create a database of high-value statements for your project and then do it by extracting relevant statements into a pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Analyzing the Relative Attachment of Subjects + Verbs / Objects</h3>\n",
    "\n",
    "We will be looking at an article pertinent to FPI.\n",
    "\n",
    "To look at high-value statements I extracted relevant statements by:<br>\n",
    "1) identifying subjects to look at political actors of interest; and the<br>\n",
    "2) verbs / objects to understand how these actors behave (verbs) and what are the objects the are acting upon.\n",
    "\n",
    "**Findings:**<br>\n",
    "<i>The subject value count performed below mentions 'demonstrators', which signals that we are looking at an event of demonstration. From the 10 sentences that I have analyzed I looked at 2 subjects 'Ahok', a governor in Indonesian capital city, and the 'demonstrators' (FPI members). From the analysis we could see that Ahok is of a Chinese descent (an important factor, as FPI framed him as a 'foreigner' and thus is not fit to govern the capital city) and the word 'must ousted' reflects the demonstrants' demand upon Ahok. Furthermore, a verb related to the demonstrators is 'were received by', which shows how the government attempted to listen to their demands in a peaceful manner instead of breaking the demonstration.\n",
    "\n",
    "This exchange revealed by the information extraction shows the social game played by FPI and the government: FPI's contentious politics against the government.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OpenIE run\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 18.047 (s)\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [19.7 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator natlog\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie\n",
      "[main] INFO edu.stanford.nlp.naturalli.ClauseSplitter - Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.0102 seconds]\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - Processing file: /var/folders/7j/8jdgq3nd7_b1h51vzjsqgfhw0000gn/T/tmpw7ctelpc\n",
      "[main] WARN edu.stanford.nlp.process.PTBLexer - Untokenizable:  (U+9D, decimal: 157)\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - All files have been queued; awaiting termination...\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - DONE processing files. 0 exceptions encountered.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ie_DF = stanford.openIE(FPIRandom['Article'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certainty</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Jakarta governor Basuki</td>\n",
       "      <td>' ÛÏAhok is</td>\n",
       "      <td>Ûªs letter recommending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>his ministry</td>\n",
       "      <td>follow up on</td>\n",
       "      <td>letter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>ÛÏI haven Ûªt</td>\n",
       "      <td>read</td>\n",
       "      <td>letter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.854793</td>\n",
       "      <td>Ahok</td>\n",
       "      <td>Christian of</td>\n",
       "      <td>Chinese descent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Ahok</td>\n",
       "      <td>said On</td>\n",
       "      <td>Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Ahok</td>\n",
       "      <td>Christian of</td>\n",
       "      <td>Chinese descent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>devil ' Ûªs</td>\n",
       "      <td>is spawn such as</td>\n",
       "      <td>ÛÏAhok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>ÛÏArrogant Ahok</td>\n",
       "      <td>must</td>\n",
       "      <td>must ousted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Ahok</td>\n",
       "      <td>must</td>\n",
       "      <td>must ousted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>were received by</td>\n",
       "      <td>Abraham ' ÛÏLulung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>were</td>\n",
       "      <td>received</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>were received by</td>\n",
       "      <td>Abraham ' ÛÏLulung respectively</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Û Lunggana</td>\n",
       "      <td>speakers from</td>\n",
       "      <td>United Development Party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Lulung</td>\n",
       "      <td>expressed</td>\n",
       "      <td>their support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Taufik</td>\n",
       "      <td>expressed</td>\n",
       "      <td>their support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Lulung</td>\n",
       "      <td>expressed</td>\n",
       "      <td>their support for demonstration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Taufik</td>\n",
       "      <td>expressed</td>\n",
       "      <td>their support for demonstration</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    certainty                  subject              verb  \\\n",
       "0    1.000000  Jakarta governor Basuki       ' ÛÏAhok is   \n",
       "1    1.000000             his ministry      follow up on   \n",
       "2    1.000000            ÛÏI haven Ûªt              read   \n",
       "3    0.854793                     Ahok      Christian of   \n",
       "4    1.000000                     Ahok           said On   \n",
       "5    1.000000                     Ahok      Christian of   \n",
       "6    1.000000              devil ' Ûªs  is spawn such as   \n",
       "7    1.000000          ÛÏArrogant Ahok              must   \n",
       "8    1.000000                     Ahok              must   \n",
       "9    1.000000            demonstrators  were received by   \n",
       "10   1.000000            demonstrators              were   \n",
       "11   1.000000            demonstrators  were received by   \n",
       "12   1.000000               Û Lunggana     speakers from   \n",
       "13   1.000000                   Lulung         expressed   \n",
       "14   1.000000                   Taufik         expressed   \n",
       "15   1.000000                   Lulung         expressed   \n",
       "16   1.000000                   Taufik         expressed   \n",
       "\n",
       "                             object  \n",
       "0           Ûªs letter recommending  \n",
       "1                            letter  \n",
       "2                            letter  \n",
       "3                   Chinese descent  \n",
       "4                            Monday  \n",
       "5                   Chinese descent  \n",
       "6                            ÛÏAhok  \n",
       "7                       must ousted  \n",
       "8                       must ousted  \n",
       "9                Abraham ' ÛÏLulung  \n",
       "10                         received  \n",
       "11  Abraham ' ÛÏLulung respectively  \n",
       "12         United Development Party  \n",
       "13                    their support  \n",
       "14                    their support  \n",
       "15  their support for demonstration  \n",
       "16  their support for demonstration  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ie_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(FPIRandom['sentences'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(s) for s in FPIRandom['sentences'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ahok                       4\n",
       "demonstrators              3\n",
       "Lulung                     2\n",
       "Taufik                     2\n",
       "his ministry               1\n",
       "Jakarta governor Basuki    1\n",
       "Û Lunggana                 1\n",
       "ÛÏI haven Ûªt              1\n",
       "ÛÏArrogant Ahok            1\n",
       "devil ' Ûªs                1\n",
       "Name: subject, dtype: int64"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ie_DF['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chinese descent    2\n",
       "must ousted        1\n",
       "Monday             1\n",
       "Name: object, dtype: int64"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ie_DF[ie_DF['subject'] == 'Ahok']['object'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Christian of    2\n",
       "said On         1\n",
       "must            1\n",
       "Name: verb, dtype: int64"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ie_DF[ie_DF['subject'] == 'Ahok']['verb'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Abraham ' ÛÏLulung respectively    1\n",
       "Abraham ' ÛÏLulung                 1\n",
       "received                           1\n",
       "Name: object, dtype: int64"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ie_DF[ie_DF['subject'] == 'demonstrators']['object'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "were received by    2\n",
       "were                1\n",
       "Name: verb, dtype: int64"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ie_DF[ie_DF['subject'] == 'demonstrators']['verb'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
